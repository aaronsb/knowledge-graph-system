# Ollama Local Inference Service
# ADR-042: Local LLM Inference for Concept Extraction
#
# Hardware-optimized profiles for different GPU types:
#   - nvidia:     NVIDIA GPUs (RTX, Tesla, A100, etc.)
#   - amd:        AMD GPUs (RX 7000 series, MI series via ROCm)
#   - intel:      Intel GPUs (Arc, Iris Xe)
#   - cpu:        CPU-only inference (no GPU)
#
# Usage:
#   # NVIDIA GPU (most common)
#   docker-compose -f docker-compose.ollama.yml --profile nvidia up -d
#
#   # AMD GPU
#   docker-compose -f docker-compose.ollama.yml --profile amd up -d
#
#   # Intel GPU
#   docker-compose -f docker-compose.ollama.yml --profile intel up -d
#
#   # CPU only
#   docker-compose -f docker-compose.ollama.yml --profile cpu up -d

services:
  # ============================================================================
  # NVIDIA GPU Variant (RTX, Tesla, A100, etc.)
  # ============================================================================
  ollama-nvidia:
    image: ollama/ollama:latest
    container_name: kg-ollama
    profiles:
      - nvidia
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    environment:
      # Ollama configuration
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=2         # Run 2 requests in parallel
      - OLLAMA_MAX_LOADED_MODELS=1    # Keep 1 model in VRAM
      - OLLAMA_KEEP_ALIVE=5m          # Keep model loaded for 5 minutes
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all                 # Use all available NVIDIA GPUs
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - knowledge-graph

  # ============================================================================
  # AMD GPU Variant (ROCm-based: RX 7000, MI series)
  # ============================================================================
  ollama-amd:
    image: ollama/ollama:rocm
    container_name: kg-ollama
    profiles:
      - amd
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    devices:
      - /dev/kfd:/dev/kfd            # Kernel Fusion Driver
      - /dev/dri:/dev/dri            # Direct Rendering Infrastructure
    group_add:
      - video
      - render
    environment:
      # ROCm configuration
      - HSA_OVERRIDE_GFX_VERSION=11.0.0  # Set for your GPU (e.g., 11.0.0 for gfx1100)
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_KEEP_ALIVE=5m
      # ROCm specific
      - ROC_ENABLE_PRE_VEGA=0
    restart: unless-stopped
    networks:
      - knowledge-graph

  # ============================================================================
  # Intel GPU Variant (Arc, Iris Xe, Data Center GPU Max)
  # ============================================================================
  ollama-intel:
    image: ollama/ollama:latest
    container_name: kg-ollama
    profiles:
      - intel
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    devices:
      - /dev/dri:/dev/dri            # Intel GPU device
    environment:
      # Intel GPU configuration (oneAPI/Level Zero)
      - OLLAMA_INTEL_GPU=1
      - NEOReadDebugKeys=1
      - ClDeviceGlobalMemSizeAvailablePercent=100
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_KEEP_ALIVE=5m
    restart: unless-stopped
    networks:
      - knowledge-graph

  # ============================================================================
  # CPU-Only Variant (AVX2/AVX512 optimized)
  # ============================================================================
  ollama-cpu:
    image: ollama/ollama:latest
    container_name: kg-ollama
    profiles:
      - cpu
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    environment:
      # CPU inference configuration
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_THREADS=8         # Use 8 CPU threads (adjust for your CPU)
      - OLLAMA_NUM_PARALLEL=1        # Single request at a time for CPU
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_USE_MMAP=true         # Memory-mapped model loading (faster startup)
    deploy:
      resources:
        limits:
          cpus: '8'                  # Limit to 8 CPU cores
          memory: 16G                # Limit to 16GB RAM
    restart: unless-stopped
    networks:
      - knowledge-graph

volumes:
  ollama-models:
    driver: local

networks:
  knowledge-graph:
    name: knowledge-graph
    driver: bridge
