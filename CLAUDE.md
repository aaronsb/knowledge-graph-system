# Knowledge Graph System - Claude Development Guide

## Project Overview

A multi-dimensional knowledge extraction system that transforms documents into interconnected concept graphs using Apache AGE (PostgreSQL graph extension), enabling semantic exploration beyond sequential reading.

**Platform Architecture (Fully Containerized):**

The system runs entirely in Docker containers - **no local Python installation required**. Configure secrets once, start infrastructure, configure through operator container.

**Five Core Containers:**
1. **knowledge-graph-postgres** - Apache AGE (PostgreSQL 16) graph database
2. **kg-api-dev** - Python FastAPI REST API server (ingestion + queries)
3. **kg-operator** - Platform configuration container (admin, AI providers, API keys)
4. **knowledge-graph-garage** - S3-compatible object storage (Garage)
5. **kg-web-dev** - React visualization web app (Next.js)

**Optional Local Clients:**
- **kg CLI** - TypeScript command-line interface (optional, for local operations)
- **MCP Server** - Model Context Protocol server for Claude Desktop integration

**Workflow:**
```
1. First-time setup:  ./operator.sh init    # Generates secrets, starts all containers, prompts for config
2. Daily start:       ./operator.sh start   # Uses saved configuration
3. Configure:         ./operator.sh shell   # Enter operator container for detailed config
4. Stop:              ./operator.sh stop    # Stop all containers
5. Use kg CLI or web UI
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Docker Containers                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Documents â†’ [API Container] â†’ LLM Extraction â†’ [Postgres]  â”‚
â”‚                      â†“                               â†“      â”‚
â”‚              [Garage S3 Storage]         [Apache AGE Graph] â”‚
â”‚                      â†“                               â†“      â”‚
â”‚               [Web Container] â† REST API â† [API Container]  â”‚
â”‚                                                             â”‚
â”‚  Configuration: [Operator Container] â†’ [Postgres Config]    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘
                   kg CLI (optional)
                   MCP Server (optional)
```

**Tech Stack:**
- Python 3.11+ FastAPI (REST API server + ingestion pipeline)
- Apache AGE / PostgreSQL 16 (graph database using openCypher)
- TypeScript/Node.js (CLI + MCP client - optional local tools)
- React/Next.js (web visualization)
- Garage (S3-compatible object storage)
- Docker Compose (infrastructure orchestration)
- OpenAI/Anthropic/Ollama APIs (LLM providers)

**Query Language:**
- **openCypher:** Open-source graph query language implemented by Apache AGE
- **ISO/IEC 39075:2024 GQL:** Standardized graph query language based on openCypher
- **Important:** AGE uses openCypher, not Neo4j's proprietary Cypher implementation
  - This explains syntax differences (e.g., no `ON CREATE SET` / `ON MATCH SET`)
  - See ADR-016 "openCypher Compatibility" section for details

## Important Files & Directories

### Configuration
- `.env` - Infrastructure secrets (generated by `./operator.sh init`, never edit manually)
- `.operator.conf` - Platform settings (dev mode, GPU mode)
- `docker/docker-compose.yml` - All container definitions
- `schema/00_baseline.sql` - Apache AGE baseline schema
- `schema/migrations/` - Database migration files

### Core Code (API Server)
- `api/api/main.py` - FastAPI application entry point
- `api/api/lib/ai_providers.py` - Modular AI provider abstraction
- `api/api/lib/llm_extractor.py` - LLM concept extraction
- `api/api/lib/age_client.py` - Apache AGE database operations
- `api/api/lib/ingestion.py` - Ingestion pipeline
- `api/api/routes/` - REST API endpoints (queries, jobs, ontology, admin)

### Core Code (TypeScript Client)
- `client/src/index.ts` - Unified CLI entry point
- `client/src/cli/` - CLI command implementations
- `client/src/api/client.ts` - REST API client
- `client/install.sh` - Installs `kg` command globally

### Core Code (Web App)
- `web/` - React/Next.js visualization application

### Operator (Platform Management)
- `operator.sh` - **Main entry point** for platform lifecycle (init, start, stop, etc.)
- `operator/configure.py` - Configuration CLI (runs inside operator container)
- `operator/lib/` - Shell scripts for lifecycle operations (called by operator.sh)
- `operator/admin/` - Admin utilities and management scripts
- `scripts/development/diagnostics/` - Diagnostic and maintenance tools

### Development Tools (Local)
- `scripts/development/test/` - Unit and integration test scripts
- `client/` - TypeScript kg CLI and MCP server source code

### Documentation
- `docs/README.md` - Documentation index
- `docs/architecture/ARCHITECTURE.md` - System design
- `docs/architecture/ADR-*.md` - Architecture Decision Records
- `docs/guides/QUICKSTART.md` - Getting started guide

## Development Workflow

### Initial Setup (First Time)

The system uses the **operator architecture (ADR-061)** where all configuration is managed through a dedicated operator container. **No local Python installation required** - everything runs in containers.

**Prerequisites:**
- Docker or Podman
- Docker Compose or Podman Compose
- Node.js + npm (to install kg CLI for convenient local operations)

**Setup Steps:**

```bash
# One command does everything:
./operator.sh init

# The init wizard will:
# 1. Generate infrastructure secrets (.env)
# 2. Detect your GPU (NVIDIA/Mac/CPU)
# 3. Start all containers
# 4. Prompt for admin password
# 5. Prompt for AI provider configuration
```

**After init completes:**
```bash
# Check status
./operator.sh status

# Optionally install kg CLI for convenient local operations
cd client && ./install.sh && cd ..

# Verify system is ready
kg health
kg database stats
```

**That's it!** No venv, no pip install, no local Python setup. Everything runs in containers.

**For manual configuration** (if you skip prompts during init):
```bash
# Enter operator shell
./operator.sh shell

# Inside the shell:
configure.py admin                           # Create admin user
configure.py ai-provider openai --model gpt-4o  # Set AI provider
configure.py embedding 2                     # Activate local embeddings
configure.py api-key openai                  # Store API key
configure.py status                          # Verify configuration
exit
```

**Using the Platform:**
- **kg CLI** (recommended) - Fast, convenient command-line interface for all operations
- **Web UI** - Interactive graph visualization at http://localhost:3000
- **Direct API** - REST endpoints at http://localhost:8000 (see /docs for OpenAPI)

### Daily Development

**Starting Services:**
```bash
# Start the platform (uses saved config from init)
./operator.sh start

# Verify everything is ready
kg health
```

**Common kg CLI Operations:**
```bash
# View platform status
kg database stats
kg admin status

# Search concepts
kg search query "recursive depth"
kg search details <concept-id>

# Ingest documents
kg ingest file -o "My Ontology" document.txt
kg ingest url -o "Research" https://example.com/article.html

# Manage ontologies
kg ontology list
kg ontology stats "My Ontology"

# Manage API keys and configuration
kg admin embedding list
kg admin extraction info
```

**Stopping Services:**
```bash
# Stop all services (keeps data)
./operator.sh stop

# Stop app only, keep database running
./operator.sh stop --keep-infra

# Complete teardown (removes containers, keeps volumes and .env)
./operator.sh teardown

# Teardown including volumes (but keep .env secrets)
./operator.sh teardown --keep-env

# Complete reset (removes everything including .env)
./operator.sh teardown --full
```

**Checking Logs:**
```bash
./operator.sh logs api      # API server logs
./operator.sh logs postgres # Database logs
./operator.sh logs web      # Web app logs
```

**Direct Database Queries:**
```bash
# No TTY required - works in scripts and automation
./operator.sh query 'SELECT count(*) FROM pg_stat_activity'
./operator.sh pg 'SHOW max_connections'  # pg is an alias

# Multi-line queries work with single quotes
./operator.sh query '
  SELECT state, count(*)
  FROM pg_stat_activity
  GROUP BY state
'
```

### Resetting Database (Clean State)

When testing or after breaking changes, you can completely reset:

```bash
# Complete reset (keeps secrets, removes all data)
./operator.sh teardown --keep-env
./operator.sh init

# Or full reset (regenerates everything)
./operator.sh teardown --full
./operator.sh init
```

### Making Changes

**When modifying AI extraction:**
1. Edit `api/api/lib/ai_providers.py` or `api/api/lib/llm_extractor.py`
2. Restart API: `./operator.sh restart api`
3. Test: `./operator.sh shell` then `python /workspace/operator/admin/extraction_test.py`
4. Test ingestion: `kg ingest file -o "Test" test-file.txt`

**When modifying database schema:**
1. Create new migration file: `schema/migrations/NNN_descriptive_name.sql`
2. Restart database (migrations auto-apply): `./operator.sh restart postgres`
3. Verify via operator shell: `./operator.sh shell` then `pg -c "\dt"`
4. **Important**: Migrations are applied automatically on start - never apply manually

**When modifying API endpoints:**
1. Edit files in `api/api/routes/`
2. Restart API: `./operator.sh restart api`
3. Test with kg CLI or curl

**When modifying kg CLI:**
1. Edit files in `client/src/cli/`
2. Rebuild: `cd client && npm run build`
3. Reinstall: `cd client && ./install.sh`
4. Test: `kg <command>`

**When modifying web app:**
1. Edit files in `web/`
2. Restart web: `./operator.sh restart web`
3. Test: Open `http://localhost:3000`

**When modifying operator:**
1. Edit `operator/configure.py` or `operator/admin/` scripts
2. Rebuild operator: `./operator.sh rebuild operator`
3. Test: `./operator.sh shell` then `configure.py status`

### Release Process

The project uses a **release branch workflow** with automatic container builds and versioning.

**Release Workflow:**

```bash
# 1. Development on main branch
git checkout main
# ... make changes, commit, push ...

# 2. Update VERSION file when ready to release
echo "0.3.0" > VERSION
git add VERSION
git commit -m "chore: bump version to 0.3.0"
git push origin main

# 3. Merge to release branch (triggers automated build)
git checkout release
git merge main
git push origin release

# â†’ GitHub Actions automatically:
#    - Reads VERSION file (0.3.0)
#    - Creates git tag (v0.3.0)
#    - Builds all 3 container images
#    - Publishes to GHCR with tags: latest, 0.3.0, 0.3
```

**When Builds Trigger:**

1. âœ… **Push to release branch** - Reads VERSION file, creates tag, builds images
2. âœ… **Push version tag** (v*.*.* ) - Builds images (for manual tagging)
3. âœ… **Pull request to main** - Builds without pushing (validation only)
4. âœ… **Manual trigger** - Via GitHub Actions UI
5. âŒ **Push to main** - Does NOT trigger build (prevents constant rebuilds)

**Published Images:**

After successful release, images are available at:
```
ghcr.io/aaronsb/knowledge-graph-system/kg-api:latest
ghcr.io/aaronsb/knowledge-graph-system/kg-api:0.3.0
ghcr.io/aaronsb/knowledge-graph-system/kg-api:0.3

ghcr.io/aaronsb/knowledge-graph-system/kg-web:latest
ghcr.io/aaronsb/knowledge-graph-system/kg-web:0.3.0

ghcr.io/aaronsb/knowledge-graph-system/kg-operator:latest
ghcr.io/aaronsb/knowledge-graph-system/kg-operator:0.3.0
```

**Version Numbering:**
- Use semantic versioning: `MAJOR.MINOR.PATCH`
- MAJOR: Breaking changes, incompatible API changes
- MINOR: New features, backward-compatible
- PATCH: Bug fixes, backward-compatible

**Maintaining the CHANGELOG:**

The project maintains `CHANGELOG.md` following [Keep a Changelog](https://keepachangelog.com) format.

**When merging PRs:**
1. **Update CHANGELOG.md** in the `[Unreleased]` section
2. **Document notable changes:** Features, fixes, breaking changes, deprecations
3. **Link to PR:** Include PR number for reference (e.g., `[#153]`)
4. **Preserve implementation details:** For squash-merged PRs, add a comment to the PR documenting the individual commits
5. **Commit the update:** Include CHANGELOG update in your PR or as a follow-up commit

**Example CHANGELOG entry:**
```markdown
## [Unreleased]

### Added

- **Feature Name** ([#123](link-to-pr))
  - Brief description of what was added
  - Key capabilities or changes
  - Performance characteristics if relevant
  - Link to detailed implementation history if PR was squash-merged
```

**When releasing:**
1. Move `[Unreleased]` entries to new version section (e.g., `[0.4.0] - 2025-11-30`)
2. Create new empty `[Unreleased]` section
3. Commit: `git commit -m "docs: update CHANGELOG for v0.4.0 release"`

**Why this matters:**
- GitHub squash merges combine all commits into one, losing granular history
- CHANGELOG preserves the implementation story and decision rationale
- Future maintainers can understand why changes were made
- Release notes can be generated from CHANGELOG

**Alternative: Manual Release Workflow**

For more control, you can manually trigger the create-release workflow:

```bash
# Trigger via GitHub CLI
gh workflow run create-release.yml -f version=patch  # 0.2.0 â†’ 0.2.1
gh workflow run create-release.yml -f version=minor  # 0.2.1 â†’ 0.3.0
gh workflow run create-release.yml -f version=major  # 0.3.0 â†’ 1.0.0
gh workflow run create-release.yml -f version=1.5.0  # Explicit version
```

This workflow:
- Calculates next version (or uses explicit version)
- Creates git tag
- Generates changelog from commit history
- Creates GitHub Release
- Triggers container builds

**Using Released Images:**

```bash
# Use published images instead of building locally
cd docker
docker-compose -f docker-compose.yml -f docker-compose.ghcr.yml up -d

# Or pull specific versions
docker pull ghcr.io/aaronsb/knowledge-graph-system/kg-api:0.3.0
```

## Key Concepts

### Concept Extraction Flow

1. **Submit** document via kg CLI â†’ POST `/ingest` endpoint
2. **Job created** with cost estimate â†’ Requires approval (ADR-014)
3. **Chunk** document into semantic chunks (~1000 words)
4. **Extract** concepts using LLM (GPT-4 or Claude) per chunk
5. **Match** against existing concepts via vector similarity
6. **Upsert** to Apache AGE with relationships
7. **Stream** progress updates via job status endpoint

### Graph Data Model

```cypher
// Nodes
(:Concept {concept_id, label, embedding, search_terms, description})
(:Source {source_id, document, paragraph, full_text})
(:Instance {instance_id, quote})

// Relationships
(:Concept)-[:APPEARS_IN]->(:Source)
(:Concept)-[:EVIDENCED_BY]->(:Instance)
(:Instance)-[:FROM_SOURCE]->(:Source)
(:Concept)-[:IMPLIES|SUPPORTS|CONTRADICTS|ENABLES]->(:Concept)
```

### AI Provider System

**Available Providers:**
- **OpenAI** - Cloud API (GPT-4o, GPT-4o-mini)
- **Anthropic** - Cloud API (Claude Sonnet 4, Claude 3.5 Sonnet)
- **Ollama** - Local inference (Mistral, Llama, Qwen, etc.) - ADR-042

**Configuration via Operator:**
```bash
# List available extraction providers and models
docker exec kg-operator python /workspace/operator/configure.py ai-provider --help

# Set OpenAI
docker exec kg-operator python /workspace/operator/configure.py ai-provider openai --model gpt-4o

# Set Anthropic
docker exec kg-operator python /workspace/operator/configure.py ai-provider anthropic --model claude-sonnet-4-20250514

# Set Ollama (requires Ollama container running)
docker exec kg-operator python /workspace/operator/configure.py ai-provider ollama --model mistral:7b-instruct

# Store API key (encrypted in database)
docker exec -it kg-operator python /workspace/operator/configure.py api-key openai

# Test extraction
docker exec kg-operator python /workspace/operator/admin/extraction_test.py
```

**Embedding Providers:**
```bash
# List available embedding profiles
docker exec kg-operator python /workspace/operator/configure.py embedding

# Typical output:
#   [1] âœ“ ACTIVE   openai   - text-embedding-3-small (1536 dims, float32)
#   [2]            local    - nomic-ai/nomic-embed-text-v1.5 (768 dims, float16, cpu)

# Activate a profile (by ID)
docker exec kg-operator python /workspace/operator/configure.py embedding 2
```

**Resource Management (ADR-043):**
When using local inference with Ollama + local embeddings on single-GPU systems, the system automatically manages VRAM contention:
- **Sufficient VRAM (>500MB free):** Embeddings run on GPU (~1-2ms per concept)
- **VRAM contention (<500MB free):** Embeddings fall back to CPU (~5-10ms per concept)
- **Performance impact:** ~100ms per chunk (negligible in 2-3 minute extraction jobs)
- **User notification:** Clear warning logs when CPU fallback activated

## Query Safety & GraphQueryFacade

**ADR-048** introduces namespace safety to prevent catastrophic collisions when vocabulary metadata moves to the graph alongside concepts.

### The Problem

Without explicit labels, queries can operate on wrong namespace:

```python
# âŒ UNSAFE: Will count ALL nodes (concepts + vocabulary)
client._execute_cypher("MATCH (n) RETURN count(n)")

# âŒ UNSAFE: Will delete vocabulary nodes too!
client._execute_cypher("MATCH (n) DELETE n")
```

### The Solution: GraphQueryFacade

Use `client.facade` for namespace-safe queries:

```python
from src.api.lib.age_client import AGEClient

client = AGEClient()

# âœ… SAFE: Only counts concept nodes
concept_count = client.facade.count_concepts()

# âœ… SAFE: Only matches concepts
concepts = client.facade.match_concepts(
    where="c.label =~ '(?i).*recursive.*'",
    limit=10
)

# âœ… SAFE: Only matches vocabulary types
vocab_types = client.facade.match_vocab_types(
    where="v.is_active = true"
)

# âœ… SAFE: Namespace-aware statistics
stats = client.facade.get_graph_stats()
```

### Facade Methods

**Concept Namespace:**
- `match_concepts(where, params, limit)` - Match :Concept nodes
- `match_concept_relationships(rel_types, where)` - Match concept edges
- `count_concepts(where, params)` - Count concepts
- `match_sources(where, params, limit)` - Match :Source nodes
- `match_instances(where, params, limit)` - Match :Instance nodes

**Vocabulary Namespace:**
- `match_vocab_types(where, params, limit)` - Match :VocabType nodes
- `match_vocab_categories(where, params)` - Match :VocabCategory nodes
- `find_vocabulary_synonyms(min_similarity, category)` - Find synonyms
- `count_vocab_types(where, params)` - Count vocabulary types

**Utilities:**
- `get_graph_stats()` - Namespace-aware statistics
- `execute_raw(query, params, namespace)` - Escape hatch for complex queries
- `get_audit_stats()` - Query safety metrics

### Query Linter

CI enforces query safety via linter:

```bash
# Run locally (via operator container)
docker exec kg-operator python /workspace/scripts/development/diagnostics/lint_queries.py --verbose

# Check specific paths
docker exec kg-operator python /workspace/scripts/development/diagnostics/lint_queries.py /workspace/src/api/routes /workspace/src/api/workers
```

**Current baseline:** 3 unsafe queries (documented in `docs/architecture/QUERY_SAFETY_BASELINE.md`)

### Development Guidelines

**When writing new code:**
1. Always use `client.facade` for graph queries
2. Never use bare `MATCH (n)` without explicit label
3. Run linter before committing: `docker exec kg-operator python /workspace/scripts/development/diagnostics/lint_queries.py`

**When fixing unsafe queries:**
```python
# Before (unsafe)
results = client._execute_cypher(
    "MATCH (n) WHERE n.property = $value RETURN n",
    params={"value": "foo"}
)

# After (safe)
results = client.facade.match_concepts(
    where="c.property = $value",
    params={"value": "foo"}
)
```

## Common Tasks

### View Platform Configuration

```bash
# See all configuration status
docker exec kg-operator python /workspace/operator/configure.py status

# Output example:
# ğŸ“Š Platform Configuration Status
# Admin users: 1
# AI Extraction: openai / gpt-4o
# Embedding: local / nomic-ai/nomic-embed-text-v1.5 (768 dims)
# API Keys: openai
```

### Manage API Keys

```bash
# Store encrypted API key
docker exec -it kg-operator python /workspace/operator/configure.py api-key openai

# View stored keys (hashes only)
docker exec kg-operator python /workspace/operator/admin/manage_api_keys.py list

# Delete API key
docker exec kg-operator python /workspace/operator/admin/manage_api_keys.py delete openai
```

### Measure Epistemic Status for Vocabulary Types

Measure epistemic status patterns for vocabulary relationship types (ADR-065):

```bash
# Measure current state (default: 100 edge sample per type)
kg vocab epistemic-status measure

# Larger sample for more precision
kg vocab epistemic-status measure --sample-size 500

# Detailed output with uncertainty metrics
kg vocab epistemic-status measure --verbose

# Analysis only (don't store results)
kg vocab epistemic-status measure --no-store
```

**Epistemic Status (Estimated from Measurements):**
- **AFFIRMATIVE** - Consistently high grounding (avg > 0.8)
- **CONTESTED** - Mixed grounding (0.2 â‰¤ avg â‰¤ 0.8)
- **CONTRADICTORY** - Consistently low/negative grounding (avg < -0.5)
- **HISTORICAL** - Temporal vocabulary (detected by name)

**Philosophy:**
- Grounding calculated dynamically at query time (bounded locality + satisficing)
- Sample-based measurement (not exhaustive analysis)
- Results are temporal and observer-dependent
- Each run is a measurement, not a classification

### Add a New AI Provider

1. Create class extending `AIProvider` in `api/api/lib/ai_providers.py`
2. Implement required methods:
   - `extract_concepts()`
   - `generate_embedding()`
   - `validate_api_key()`
   - `list_available_models()`
3. Update `get_provider()` factory function in `api/api/lib/ai_providers.py`
4. Add configuration option in `operator/configure.py`
5. Test: `./operator.sh shell` then `python /workspace/operator/admin/extraction_test.py`

### Add a New API Endpoint

1. Create route handler in `api/api/routes/` (e.g., `queries.py`)
2. Add request/response Pydantic models in route file
3. Implement AGEClient method in `api/api/lib/age_client.py`
4. Add endpoint to router in `api/api/main.py`
5. Restart API: `./operator.sh restart api`
6. Test with curl or kg CLI

### Add a New kg CLI Command

1. Create command file in `client/src/cli/` (e.g., `search.ts`)
2. Add client method in `client/src/api/client.ts`
3. Register command in `client/src/index.ts`
4. Rebuild: `cd client && npm run build`
5. Reinstall: `cd client && ./install.sh`
6. Test: `kg <new-command>`

### Add New Database Migration

1. Create migration file: `schema/migrations/NNN_descriptive_name.sql`
   - Use next sequential number (check existing migrations)
   - Follow existing migration format
2. Migration auto-applies on next database start
3. Verify via operator shell: `./operator.sh shell` then `pg -c "\dt"`

### Create a New ADR (Architecture Decision Record)

When making significant architectural decisions:

1. Create new ADR file: `docs/architecture/ADR-###-descriptive-name.md`
   - Use the next sequential number (check existing ADRs)
   - Follow the format from existing ADRs
   - Include: Status, Date, Context, Decision, Consequences, Alternatives Considered
2. **IMPORTANT:** Update `docs/architecture/ARCHITECTURE_DECISIONS.md`
   - Add new entry to the ADR Index table with title, status, and brief summary
   - Update "Last Updated" date
   - This file serves as the master index for all ADRs
3. Reference the new ADR in related documentation:
   - Update `docs/README.md` if it's a major user-facing change
   - Link from related ADRs using "Related ADRs" section
   - Update CLAUDE.md if it affects development workflow

## Troubleshooting

### Infrastructure Won't Start

```bash
# Check container status
./operator.sh status

# Check logs
./operator.sh logs postgres
./operator.sh logs garage
./operator.sh logs operator

# Check if .env exists and has secrets
cat .env | grep -E "ENCRYPTION_KEY|POSTGRES_PASSWORD|OAUTH_SIGNING_KEY"

# Complete restart (keeps secrets)
./operator.sh teardown --keep-env
./operator.sh init
```

### API Server Issues

```bash
# Check API status
curl http://localhost:8000/health
kg health

# Check API logs
./operator.sh logs api

# Restart API
./operator.sh restart api

# Check API configuration
./operator.sh shell
configure.py status
exit
```

### Database Connection Issues

```bash
# Check postgres container
./operator.sh status

# Check postgres logs
./operator.sh logs postgres

# Direct SQL queries (no TTY required - works in scripts/automation)
./operator.sh query 'SELECT count(*) FROM pg_stat_activity'
./operator.sh query 'SHOW max_connections'
./operator.sh pg '\dt'                    # pg is an alias for query

# For interactive exploration, use operator shell
./operator.sh shell
pg -l                    # List databases
pg -c "\dx"              # Check AGE extension loaded
pg -c "\dt"              # Check tables exist
exit
```

### Configuration Issues

```bash
# Enter operator shell
./operator.sh shell

# Quick status check
status                                # Alias for configure.py status

# Or detailed commands:
configure.py status                   # Full platform status
configure.py embedding                # List embedding profiles

# Test AI extraction
python /workspace/operator/admin/extraction_test.py

# List API keys (hashes only)
python /workspace/operator/admin/manage_api_keys.py list

exit
```

### kg CLI Issues

```bash
# Check kg CLI installed
which kg
kg --version

# Check API connection
kg health

# Reinstall kg CLI
cd client
npm install
npm run build
./install.sh
```

### Web App Issues

```bash
# Check web container
./operator.sh logs web

# Check web app running
curl http://localhost:3000

# Restart web app
./operator.sh restart web
```

## Code Style Guidelines

### Python (API Server)
- Use type hints for all functions
- Modular functions (<50 lines preferred)
- Package imports: `from api.api.lib.module import func`
- FastAPI route handlers in `api/api/routes/`
- Pydantic models for request/response validation

### TypeScript (kg CLI + MCP)
- Async/await for HTTP API calls
- Type interfaces for API request/response models
- Commander.js for CLI argument parsing
- Error handling with try/catch and colored output
- Axios client for REST API communication

### Shell Scripts (Operator)
- Use `set -e` for error handling
- Color codes for output clarity
- Validate prerequisites before running
- Consistent output format (â†’ for actions, âœ“ for success, âœ— for errors)

### React (Web App)
- Functional components with hooks
- TypeScript for type safety
- Component composition over inheritance
- Tailwind CSS for styling

## Testing Strategy

### Automated Tests

Development-time unit and integration tests:

```bash
# Run all tests (API + linters)
./scripts/development/test/all.sh

# Run API tests only
./scripts/development/test/api.sh

# Run linters only
./scripts/development/test/lint.sh

# Quick mode (skip coverage)
./scripts/development/test/all.sh --quick

# Pass arguments to pytest
./scripts/development/test/api.sh -v -k test_concept_extraction
```

**Note:** Test scripts automatically activate Python venv if needed. They test the API codebase, not the running containers.

### Manual Verification

```bash
# Check containers running
docker ps --format "table {{.Names}}\t{{.Status}}"

# Check platform configuration
kg admin status
docker exec kg-operator python /workspace/operator/configure.py status

# Test API health
kg health

# Test database connectivity
kg database stats

# Test ingestion pipeline
kg ingest file -o "Test" test-document.txt
kg database stats  # Verify concepts were created

# Test web app
open http://localhost:3000
```

## Performance Considerations

### Ingestion Speed
- LLM calls are slowest part (~2-5s per chunk)
- Job approval workflow (ADR-014) provides cost estimates
- Smart chunking optimizes chunk boundaries (~1000 words)
- Background job processing via FastAPI scheduler

### Query Performance
- Vector search: Python numpy cosine similarity (full scan)
- Graph traversal: AGE Cypher queries, limit depth to avoid explosion
- REST API adds ~10-50ms overhead vs direct DB queries
- Consider pgvector extension for faster vector search at scale

## Security Notes

- **Infrastructure Secrets (.env)**: Generated by `./operator.sh init`, never edit manually
  - `ENCRYPTION_KEY` - Fernet key for encrypting API keys at rest
  - `OAUTH_SIGNING_KEY` - JWT token signing key
  - `POSTGRES_PASSWORD` - Database password
  - `GARAGE_RPC_SECRET` - Garage cluster secret
  - `INTERNAL_KEY_SERVICE_SECRET` - Internal service authorization token (ADR-031)
- **Application Configuration**: Stored encrypted in PostgreSQL (ADR-031)
  - OpenAI/Anthropic API keys
  - Garage S3 credentials
  - Admin user passwords
- **API Server**: No authentication by default - add if exposing publicly
- **kg CLI**: Communicates with localhost:8000 by default
- **MCP Server**: Runs locally via Claude Desktop, no external exposure
- **Docker**: Containers on isolated network

## Resources

### Graph Database & Query Language
- Apache AGE: https://age.apache.org/
- AGE Manual: https://age.apache.org/age-manual/master/intro/overview.html
- openCypher: https://opencypher.org/
- ISO/IEC 39075 GQL Standard: https://www.iso.org/standard/76120.html
- openCypher Language Reference: https://s3.amazonaws.com/artifacts.opencypher.org/openCypher9.pdf

### Frameworks & APIs
- FastAPI: https://fastapi.tiangolo.com/
- MCP Protocol: https://spec.modelcontextprotocol.io/
- OpenAI API: https://platform.openai.com/docs
- Anthropic API: https://docs.anthropic.com/

### Architecture
- See `docs/architecture/ARCHITECTURE_DECISIONS.md` for full ADR index
- See `docs/` for complete documentation
