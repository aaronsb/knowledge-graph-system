
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Multi-dimensional knowledge extraction system using Apache AGE">
      
      
        <meta name="author" content="Knowledge Graph System Contributors">
      
      
      
        <link rel="prev" href="../ADR-041-ai-extraction-config/">
      
      
        <link rel="next" href="../ADR-043-single-node-resource-management/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>ADR-042: Local LLM Inference for Concept Extraction - Knowledge Graph System</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Mono:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto Mono";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/css/graph-theme.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#adr-042-local-llm-inference-for-concept-extraction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Knowledge Graph System" class="md-header__button md-logo" aria-label="Knowledge Graph System" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.5 17c-.14 0-.26 0-.39.04L17.5 13.8c.45-.45.75-1.09.75-1.8a2.5 2.5 0 0 0-2.5-2.5c-.14 0-.25 0-.4.04L13.74 6.3c.47-.46.76-1.09.76-1.8a2.5 2.5 0 0 0-5 0c0 .7.29 1.34.76 1.79L8.65 9.54c-.15-.04-.26-.04-.4-.04a2.5 2.5 0 0 0-2.5 2.5c0 .71.29 1.34.75 1.79l-1.61 3.25C4.76 17 4.64 17 4.5 17a2.5 2.5 0 0 0 0 5A2.5 2.5 0 0 0 7 19.5c0-.7-.29-1.34-.76-1.79l1.62-3.25c.14.04.26.04.39.04s.25 0 .38-.04l1.63 3.25c-.47.45-.76 1.09-.76 1.79a2.5 2.5 0 0 0 5 0A2.5 2.5 0 0 0 12 17c-.13 0-.26 0-.39.04L10 13.8c.45-.45.75-1.09.75-1.8 0-.7-.29-1.33-.75-1.79l1.61-3.25c.13.04.26.04.39.04s.26 0 .39-.04L14 10.21a2.5 2.5 0 0 0 1.75 4.29c.13 0 .25 0 .38-.04l1.63 3.25c-.47.45-.76 1.09-.76 1.79a2.5 2.5 0 0 0 5 0 2.5 2.5 0 0 0-2.5-2.5m-15 3.5c-.55 0-1-.45-1-1s.45-1 1-1 1 .45 1 1-.45 1-1 1m8.5-1c0 .55-.45 1-1 1s-1-.45-1-1 .45-1 1-1 1 .45 1 1M7.25 12c0-.55.45-1 1-1s1 .45 1 1-.45 1-1 1-1-.45-1-1M11 4.5c0-.55.45-1 1-1s1 .45 1 1-.45 1-1 1-1-.45-1-1m3.75 7.5c0-.55.45-1 1-1s1 .45 1 1-.45 1-1 1-1-.45-1-1m4.75 8.5c-.55 0-1-.45-1-1s.45-1 1-1 1 .45 1 1-.45 1-1 1"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Knowledge Graph System
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ADR-042: Local LLM Inference for Concept Extraction
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/aaronsb/knowledge-graph-system" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    aaronsb/knowledge-graph-system
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Documentation Index

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../manual/" class="md-tabs__link">
          
  
  
    
  
  Manual

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../ADR-001-multi-tier-agent-access/" class="md-tabs__link">
          
  
  
    
  
  Architecture

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../development/DEV_JOURNAL_chunked_ingestion/" class="md-tabs__link">
          
  
  
    
  
  Development

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../testing/SCHEMA_MIGRATION_TEST_REPORT/" class="md-tabs__link">
          
  
  
    
  
  Testing

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Knowledge Graph System" class="md-nav__button md-logo" aria-label="Knowledge Graph System" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.5 17c-.14 0-.26 0-.39.04L17.5 13.8c.45-.45.75-1.09.75-1.8a2.5 2.5 0 0 0-2.5-2.5c-.14 0-.25 0-.4.04L13.74 6.3c.47-.46.76-1.09.76-1.8a2.5 2.5 0 0 0-5 0c0 .7.29 1.34.76 1.79L8.65 9.54c-.15-.04-.26-.04-.4-.04a2.5 2.5 0 0 0-2.5 2.5c0 .71.29 1.34.75 1.79l-1.61 3.25C4.76 17 4.64 17 4.5 17a2.5 2.5 0 0 0 0 5A2.5 2.5 0 0 0 7 19.5c0-.7-.29-1.34-.76-1.79l1.62-3.25c.14.04.26.04.39.04s.25 0 .38-.04l1.63 3.25c-.47.45-.76 1.09-.76 1.79a2.5 2.5 0 0 0 5 0A2.5 2.5 0 0 0 12 17c-.13 0-.26 0-.39.04L10 13.8c.45-.45.75-1.09.75-1.8 0-.7-.29-1.33-.75-1.79l1.61-3.25c.13.04.26.04.39.04s.26 0 .39-.04L14 10.21a2.5 2.5 0 0 0 1.75 4.29c.13 0 .25 0 .38-.04l1.63 3.25c-.47.45-.76 1.09-.76 1.79a2.5 2.5 0 0 0 5 0 2.5 2.5 0 0 0-2.5-2.5m-15 3.5c-.55 0-1-.45-1-1s.45-1 1-1 1 .45 1 1-.45 1-1 1m8.5-1c0 .55-.45 1-1 1s-1-.45-1-1 .45-1 1-1 1 .45 1 1M7.25 12c0-.55.45-1 1-1s1 .45 1 1-.45 1-1 1-1-.45-1-1M11 4.5c0-.55.45-1 1-1s1 .45 1 1-.45 1-1 1-1-.45-1-1m3.75 7.5c0-.55.45-1 1-1s1 .45 1 1-.45 1-1 1-1-.45-1-1m4.75 8.5c-.55 0-1-.45-1-1s.45-1 1-1 1 .45 1 1-.45 1-1 1"/></svg>

    </a>
    Knowledge Graph System
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/aaronsb/knowledge-graph-system" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    aaronsb/knowledge-graph-system
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Documentation Index
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../manual/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Manual
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Manual
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/00-introduction/01-WHAT_AND_WHY/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    What is the Knowledge Graph System?
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/01-getting-started/01-QUICKSTART/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quick Start Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/01-getting-started/02-CLI_USAGE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    kg CLI Usage Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/01-getting-started/03-INGESTION/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Document Ingestion Guide
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Configuration
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Configuration
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/02-configuration/01-AI_PROVIDERS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Provider Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/02-configuration/02-EXTRACTION_CONFIGURATION/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Extraction Configuration Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/02-configuration/03-EMBEDDING_CONFIGURATION/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embedding Configuration Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/02-configuration/04-SWITCHING_EXTRACTION_PROVIDERS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Switching Extraction Providers - Quick Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/02-configuration/05-LOCAL_INFERENCE_IMPLEMENTATION/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Local LLM Inference Implementation Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/02-configuration/06-EXTRACTION_QUALITY_COMPARISON/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Extraction Quality Comparison
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Integration
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Integration
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/03-integration/01-MCP_SETUP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MCP Server Setup Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/03-integration/02-VOCABULARY_CONSOLIDATION/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Edge Vocabulary Consolidation Guide
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Security & Access
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Security & Access
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/04-security-and-access/01-AUTHENTICATION/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Authentication Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/04-security-and-access/02-RBAC/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RBAC Operations Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/04-security-and-access/03-SECURITY/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Security Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/04-security-and-access/04-PASSWORD_RECOVERY/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Password Recovery and Account Management
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_7" >
        
          
          <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Maintenance
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7">
            <span class="md-nav__icon md-icon"></span>
            Maintenance
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/05-maintenance/01-BACKUP_RESTORE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backup and Restore Guide
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/05-maintenance/02-DATABASE_MIGRATIONS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Database Migrations Guide
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_8" >
        
          
          <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/01-SCHEMA_REFERENCE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PostgreSQL Schema Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/02-USE_CASES/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Use Cases: Practical Workflows
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/03-EXAMPLES/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Examples: Real Queries, Real Results
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/04-github_project_history/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GitHub Project History Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/05-CONCEPTS_AND_TERMINOLOGY/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Concepts and Terminology
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/06-CONCEPT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Concept: Why Knowledge Graphs, Not Just RAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/07-ENRICHMENT_JOURNEY/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    The Enrichment Journey: From Empty Graph to Multi-Perspective Understanding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/08-DISTRIBUTED_SHARDING_RESEARCH/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Graph Database Sharding: Research and Architectural Patterns
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/09-CYPHER_PATTERNS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Advanced Cypher Query Patterns for Apache AGE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../manual/06-reference/10-OPENCYPHER_QUERIES/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    openCypher Query Examples
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Architecture
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Architecture
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-001-multi-tier-agent-access/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-001: Multi-Tier Agent Access Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-002-node-fitness-scoring/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-002: Node Fitness Scoring System
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-003-semantic-tool-hints/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-003: Semantic Tool Hint Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-004-pure-graph-design/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-004: Pure Graph Design (Library Metaphor)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-005-source-text-tracking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-005: Source Text Tracking and Retrieval
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-006-staging-migration-workflows/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-006: Staging and Migration Workflows
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-011-cli-admin-separation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-011: CLI and Admin Tooling Separation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-012-api-server-architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-012: API Server Architecture for Scalable Neo4j Access
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-013-unified-typescript-client/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-013: Unified TypeScript Client (CLI + MCP Server)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-014-job-approval-workflow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-014: Job Approval Workflow with Pre-Ingestion Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-015-backup-restore-streaming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-015: Backup/Restore Streaming Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-016-apache-age-migration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-016: Apache AGE Migration (Neo4j Replacement)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-017-sensitive-auth-verification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Architecture Decision Record: Client-Initiated Token Revocation for Elevated Operations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-018-server-sent-events-streaming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-018: Server-Sent Events for Real-Time Progress Streaming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-019-type-based-table-formatting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-019: Type-Based Table Formatting System
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-020-admin-module-architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-020: Admin Module Architecture Pattern
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-021-live-man-switch-ai-safety/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-021: Live Man Switch - AI Safety for Critical Operations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-022-semantic-relationship-taxonomy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-022: Semantically Sparse 30-Type Relationship Taxonomy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-023-markdown-structured-content-preprocessing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-023: Markdown Structured Content Preprocessing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-024-multi-schema-postgresql-architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-024: Multi-Schema PostgreSQL Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-025-dynamic-relationship-vocabulary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-025: Dynamic Relationship Vocabulary Management
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-026-autonomous-vocabulary-curation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-026: Autonomous Vocabulary Curation and Ontology Management
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-027-user-management-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-027: User Management API with Lightweight JWT Authentication
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-028-dynamic-rbac-system/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-028: Dynamic Role-Based Access Control (RBAC) System
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-029-cli-theory-of-operation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-029: CLI Theory of Operation - Hybrid Unix/Domain-Specific Design
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-030-concept-deduplication-validation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-030: Concept Deduplication Quality Validation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-031-encrypted-api-key-storage/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-031: Encrypted API Key Storage with Container Secrets
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-032-IMPLEMENTATION-NOTES/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-032 Implementation Quick Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-032-automatic-edge-vocabulary-expansion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-032: Automatic Edge Vocabulary Expansion with Intelligent Pruning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-033-multimodal-ingestion-configurable-prompts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-033: Multimodal Image Ingestion with Configurable Prompt System
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-034-graph-visualization-query-workbench/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-034: Graph Visualization &amp; Interactive Query Explorers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-035-explorer-methods-uses-capabilities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-035: Explorer Methods, Uses, and Capabilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-036-universal-visual-query-builder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-036: Universal Visual Query Builder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-037-human-guided-graph-editing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-037: Human-Guided Graph Editing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-038-official-project-apparel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-038: Official Project Apparel Design Specifications
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-039-local-embedding-service/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-039: Local Embedding Service with Hybrid Client/Server Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-040-database-schema-migrations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-040: Database Schema Migration Management
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-041-ai-extraction-config/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-041: AI Extraction Provider Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    ADR-042: Local LLM Inference for Concept Extraction
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    ADR-042: Local LLM Inference for Concept Extraction
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#context" class="md-nav__link">
    <span class="md-ellipsis">
      Context
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Context">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#current-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Current Limitations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#current-extraction-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Current Extraction Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#success-criteria-for-local-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Success Criteria for Local Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#decision" class="md-nav__link">
    <span class="md-ellipsis">
      Decision
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Decision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architectural-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Architectural Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#technology-choices" class="md-nav__link">
    <span class="md-ellipsis">
      Technology Choices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Technology Choices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-ollama-default-local-provider" class="md-nav__link">
    <span class="md-ellipsis">
      Primary: Ollama (Default Local Provider)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#secondary-vllm-optional-enterprise" class="md-nav__link">
    <span class="md-ellipsis">
      Secondary: vLLM (Optional - Enterprise)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tertiary-llamacpp-future" class="md-nav__link">
    <span class="md-ellipsis">
      Tertiary: llama.cpp (Future)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration-schema" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Schema
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deployment-scenarios" class="md-nav__link">
    <span class="md-ellipsis">
      Deployment Scenarios
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deployment Scenarios">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-1-docker-compose-all-in-one-recommended" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 1: Docker Compose All-in-One (Recommended)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Scenario 1: Docker Compose All-in-One (Recommended)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-composeollamayml-main-ollama-config" class="md-nav__link">
    <span class="md-ellipsis">
      docker-compose.ollama.yml (Main Ollama Config)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage-by-hardware-type" class="md-nav__link">
    <span class="md-ellipsis">
      Usage by Hardware Type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#main-docker-composeyml-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Main docker-compose.yml Integration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-2-external-ollama-instance" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 2: External Ollama Instance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-3-system-wide-ollama-installation" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 3: System-Wide Ollama Installation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-4-vllm-for-enterprise" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario 4: vLLM for Enterprise
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hardware-deployment-profiles" class="md-nav__link">
    <span class="md-ellipsis">
      Hardware Deployment Profiles
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Hardware Deployment Profiles">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reference-hardware-development-machine" class="md-nav__link">
    <span class="md-ellipsis">
      Reference Hardware (Development Machine)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#profile-1-budget-cpu-only-entry-level" class="md-nav__link">
    <span class="md-ellipsis">
      Profile 1: Budget CPU-Only (Entry Level)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#profile-2-mid-range-gpu-prosumer" class="md-nav__link">
    <span class="md-ellipsis">
      Profile 2: Mid-Range GPU (Prosumer)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#profile-3-high-end-gpu-production-reference-machine" class="md-nav__link">
    <span class="md-ellipsis">
      Profile 3: High-End GPU (Production - Reference Machine)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#profile-4-professional-gpu-enterprise" class="md-nav__link">
    <span class="md-ellipsis">
      Profile 4: Professional GPU (Enterprise)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#profile-5-cloudbare-metal-hyperscale" class="md-nav__link">
    <span class="md-ellipsis">
      Profile 5: Cloud/Bare Metal (Hyperscale)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-size-recommendations-by-profile" class="md-nav__link">
    <span class="md-ellipsis">
      Model Size Recommendations by Profile
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Analysis
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Performance Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#document-ingestion-timeline" class="md-nav__link">
    <span class="md-ellipsis">
      Document Ingestion Timeline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-ingestion-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Ingestion Analysis
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implementation-plan" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-ollama-integration-mvp-week-1-2" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 1: Ollama Integration (MVP) - Week 1-2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-quality-validation-week-2-3" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 2: Quality Validation - Week 2-3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-advanced-features-week-3-4" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 3: Advanced Features - Week 3-4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-enterprise-features-future" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 4: Enterprise Features (Future)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#consequences" class="md-nav__link">
    <span class="md-ellipsis">
      Consequences
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Consequences">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#positive" class="md-nav__link">
    <span class="md-ellipsis">
      Positive
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#negative" class="md-nav__link">
    <span class="md-ellipsis">
      Negative
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#risks-mitigation" class="md-nav__link">
    <span class="md-ellipsis">
      Risks &amp; Mitigation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#security-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Security Considerations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#open-questions" class="md-nav__link">
    <span class="md-ellipsis">
      Open Questions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#success-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Success Metrics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Success Metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quality-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Quality Metrics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Metrics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adoption-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Adoption Metrics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reliability-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Reliability Metrics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#related-adrs" class="md-nav__link">
    <span class="md-ellipsis">
      Related ADRs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implementation-status" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Status
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-043-single-node-resource-management/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-043: Single-Node Resource Management for Local Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ADR-044-probabilistic-truth-convergence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ADR-044: Probabilistic Truth Convergence Through Contradiction Resolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ARCHITECTURE_DECISIONS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Architecture Decision Records (ADRs)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ARCHITECTURE_OVERVIEW/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Knowledge Graph System Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CLI_AUTHENTICATION_ARCHITECTURE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CLI Authentication Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../DATA_CONTRACT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Contract Pattern: Schema Governance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../FUZZY_MATCHING_ANALYSIS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fuzzy Matching Analysis for 30-Type Relationship Taxonomy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RECURSIVE_UPSERT_ARCHITECTURE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Knowledge Graph System - Recursive Upsert Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../visualization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Visualization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Development
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Development
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/DEV_JOURNAL_chunked_ingestion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Development Journal: Chunked Ingestion System
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/LEARNED_KNOWLEDGE_MCP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Learned Knowledge Synthesis - MCP Enhancement Plan
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/pattern-repetition-notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pattern Repetition in Growth Management
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Testing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Testing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../testing/SCHEMA_MIGRATION_TEST_REPORT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PostgreSQL Schema Migration - Functional Test Report
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../testing/TEST_COVERAGE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Test Coverage Areas
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="adr-042-local-llm-inference-for-concept-extraction">ADR-042: Local LLM Inference for Concept Extraction</h1>
<p><strong>Status:</strong> Accepted
<strong>Date:</strong> 2025-10-22
<strong>Implemented:</strong> 2025-10-22
<strong>Deciders:</strong> System Architects
<strong>Related:</strong> ADR-039 (Local Embedding Service), ADR-041 (AI Extraction Config), ADR-025 (Dynamic Relationship Vocabulary)</p>
<h2 id="context">Context</h2>
<p>Currently, the system requires cloud API access (OpenAI or Anthropic) for concept extraction during document ingestion. This creates several challenges:</p>
<h3 id="current-limitations">Current Limitations</h3>
<ol>
<li><strong>External Dependency</strong></li>
<li>System cannot function without API access</li>
<li>Network failures block ingestion</li>
<li>
<p>Subject to provider outages</p>
</li>
<li>
<p><strong>Cost Considerations</strong></p>
</li>
<li>API costs scale linearly with volume</li>
<li>Large ingestion jobs can be expensive</li>
<li>
<p>No cost ceiling for usage</p>
</li>
<li>
<p><strong>Privacy Concerns</strong></p>
</li>
<li>Sensitive documents must be sent to third-party APIs</li>
<li>Compliance issues for regulated industries (HIPAA, GDPR, etc.)</li>
<li>
<p>No air-gapped deployment possible</p>
</li>
<li>
<p><strong>Latency</strong></p>
</li>
<li>Network round-trips for each chunk (~1-3 seconds overhead)</li>
<li>Rate limiting can slow batch ingestion</li>
<li>
<p>Geographic latency for non-US regions</p>
</li>
<li>
<p><strong>Vendor Lock-in</strong></p>
</li>
<li>Tied to specific providers' model availability</li>
<li>Cannot use latest open-source models</li>
<li>Model deprecation risk</li>
</ol>
<h3 id="current-extraction-architecture">Current Extraction Architecture</h3>
<p><strong>Processing Pipeline:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>Document → Chunking → LLM Extraction → Graph Upsert
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>           (1000w)    (GPT-4o/Claude)   (PostgreSQL+AGE)
</code></pre></div></p>
<p><strong>Chunking System</strong> (<code>src/api/lib/chunker.py</code>):
- <strong>Target:</strong> 1000 words/chunk (configurable: 800-1500)
- <strong>Overlap:</strong> 200 words between chunks for context
- <strong>Smart Boundaries:</strong> Paragraph &gt; Sentence &gt; Pause &gt; Hard cut
- <strong>Average Document:</strong> 5000-50000 words = 5-50 chunks</p>
<p><strong>LLM Requirements</strong> (per chunk):
- <strong>Input Tokens:</strong> ~1500-2500 tokens
  - System prompt: ~500-700 tokens (includes relationship types)
  - Chunk text: ~1000-1500 tokens
  - Existing concepts list: 0-300 tokens (variable)
- <strong>Output Tokens:</strong> ~500-2000 tokens (JSON structure)
- <strong>Total:</strong> ~2000-4500 tokens per chunk</p>
<p><strong>Extraction Output</strong> (JSON structure):
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="p">{</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="w">  </span><span class="nt">&quot;concepts&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="w">    </span><span class="nt">&quot;concept_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;concept_001&quot;</span><span class="p">,</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="w">    </span><span class="nt">&quot;label&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Concept Name&quot;</span><span class="p">,</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="w">    </span><span class="nt">&quot;search_terms&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;term1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;term2&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;term3&quot;</span><span class="p">]</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="w">  </span><span class="p">}],</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="w">  </span><span class="nt">&quot;instances&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="w">    </span><span class="nt">&quot;concept_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;concept_001&quot;</span><span class="p">,</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span class="w">    </span><span class="nt">&quot;quote&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Exact quote from text&quot;</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span class="w">  </span><span class="p">}],</span>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="w">  </span><span class="nt">&quot;relationships&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span class="w">    </span><span class="nt">&quot;from_concept_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;concept_001&quot;</span><span class="p">,</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a><span class="w">    </span><span class="nt">&quot;to_concept_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;concept_002&quot;</span><span class="p">,</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a><span class="w">    </span><span class="nt">&quot;relationship_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;IMPLIES&quot;</span><span class="p">,</span><span class="w">  </span><span class="c1">// 30-90 dynamic types (ADR-025)</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a><span class="w">    </span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.9</span>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a><span class="w">  </span><span class="p">}]</span>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a><span class="p">}</span>
</code></pre></div></p>
<p><strong>Dynamic Vocabulary Challenge</strong> (ADR-025):
- Relationship types grow from 30 (baseline) to 30-90 (curator-approved)
- Prompt size varies: ~150 tokens (30 types) to ~450 tokens (90 types)
- Local models must handle variable-length relationship lists
- JSON structure must support any valid type from active vocabulary</p>
<h3 id="success-criteria-for-local-inference">Success Criteria for Local Inference</h3>
<ol>
<li><strong>Quality:</strong> 90-95%+ of GPT-4o extraction quality</li>
<li><strong>Reliability:</strong> 99%+ valid JSON responses</li>
<li><strong>Performance:</strong> &lt; 30 seconds per chunk (acceptable for batch ingestion)</li>
<li><strong>Resource Efficiency:</strong> Run alongside PostgreSQL and embedding model</li>
<li><strong>Deployment Simplicity:</strong> Easy installation and model management</li>
</ol>
<hr />
<h2 id="decision">Decision</h2>
<p><strong>Extend the existing extraction provider system to support local inference backends.</strong></p>
<h3 id="architectural-approach">Architectural Approach</h3>
<p>Follow the same pattern established for embeddings (ADR-039):</p>
<ol>
<li><strong>Provider Abstraction</strong></li>
<li>Add <code>ollama</code>, <code>vllm</code>, and <code>llama-cpp</code> as new provider types</li>
<li>Extend <code>ai_extraction_config</code> table to support local providers</li>
<li>
<p>Same API/CLI as existing providers (openai, anthropic)</p>
</li>
<li>
<p><strong>Configuration Pattern</strong></p>
</li>
<li>Users can choose between remote (openai, anthropic) and local (ollama, vllm) providers</li>
<li>Similar to embeddings: <code>kg admin extraction set --provider ollama --model mistral:7b-instruct</code></li>
<li>Hot reload support (if model is already loaded)</li>
<li>
<p>Provider-specific settings (base_url, temperature, etc.)</p>
</li>
<li>
<p><strong>Deployment Options</strong></p>
</li>
<li><strong>Docker Compose (Recommended):</strong> Self-contained stack with Ollama service</li>
<li><strong>External Endpoint:</strong> Point to existing local inference server</li>
<li><strong>System Installation:</strong> User installs Ollama/vLLM themselves</li>
</ol>
<h3 id="technology-choices">Technology Choices</h3>
<h4 id="primary-ollama-default-local-provider">Primary: Ollama (Default Local Provider)</h4>
<p><strong>Why Ollama:</strong></p>
<ol>
<li><strong>Simplest Deployment</strong></li>
<li>Docker image available: <code>ollama/ollama</code></li>
<li>OpenAI-compatible API (drop-in replacement)</li>
<li>Automatic model management</li>
<li>JSON mode support</li>
<li>
<p>Model listing API: <code>GET /api/tags</code></p>
</li>
<li>
<p><strong>Uses llama.cpp Under the Hood</strong></p>
</li>
<li>Ollama wraps llama.cpp for inference</li>
<li>Gets llama.cpp's performance and quantization</li>
<li>Adds management layer (download, update, list models)</li>
<li>
<p>Better API ergonomics than raw llama.cpp</p>
</li>
<li>
<p><strong>Docker Compose Integration</strong>
   <div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="nt">services</span><span class="p">:</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="w">  </span><span class="nt">ollama</span><span class="p">:</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/ollama:latest</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="w">    </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;11434:11434&quot;</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama-models:/root/.ollama</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span class="w">    </span><span class="nt">environment</span><span class="p">:</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OLLAMA_NUM_PARALLEL=2</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OLLAMA_MAX_LOADED_MODELS=1</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Flexibility</strong></p>
</li>
<li>Users can run Ollama externally and point to it</li>
<li>Or use included Docker Compose service</li>
<li>Or install Ollama system-wide</li>
<li>Model discovery via API (<code>GET /api/tags</code>)</li>
</ol>
<h4 id="secondary-vllm-optional-enterprise">Secondary: vLLM (Optional - Enterprise)</h4>
<p><strong>Why Support vLLM:</strong>
- Highest throughput for GPU deployments
- Tensor parallelism for 70B+ models
- Production-grade with load balancing
- Users may already have vLLM running</p>
<p><strong>Integration:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="c1"># docker-compose.yml (optional vLLM service)</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="nt">services</span><span class="p">:</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="w">  </span><span class="nt">vllm</span><span class="p">:</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm/vllm-openai:latest</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="w">    </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;8000:8000&quot;</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">--model meta-llama/Llama-3.1-8B-Instruct --gpu-memory-utilization 0.9</span>
</code></pre></div></p>
<h4 id="tertiary-llamacpp-future">Tertiary: llama.cpp (Future)</h4>
<p><strong>Why Consider:</strong>
- Pure CPU inference
- Extremely low resource usage
- Good for edge deployments</p>
<p><strong>Integration:</strong> Via llama-cpp-python or standalone server</p>
<h3 id="configuration-schema">Configuration Schema</h3>
<p>Extend <code>ai_extraction_config</code> table (follows embedding pattern from ADR-039):</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="c1">-- Existing columns</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">provider</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="w">       </span><span class="c1">-- &quot;openai&quot;, &quot;anthropic&quot;, &quot;ollama&quot;, &quot;vllm&quot;, &quot;llama-cpp&quot;</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">model_name</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span><span class="w">    </span><span class="c1">-- &quot;gpt-4o&quot;, &quot;claude-sonnet-4&quot;, &quot;mistral:7b-instruct&quot;</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="n">supports_vision</span><span class="w"> </span><span class="nb">BOOLEAN</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a><span class="n">supports_json_mode</span><span class="w"> </span><span class="nb">BOOLEAN</span>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="n">max_tokens</span><span class="w"> </span><span class="nb">INTEGER</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="c1">-- New columns for local providers</span>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a><span class="n">base_url</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span><span class="w">              </span><span class="c1">-- &quot;http://localhost:11434&quot; or &quot;http://ollama:11434&quot;</span>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a><span class="n">temperature</span><span class="w"> </span><span class="nb">FLOAT</span><span class="w"> </span><span class="k">DEFAULT</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="w">      </span><span class="c1">-- Lower for consistent JSON</span>
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a><span class="n">top_p</span><span class="w"> </span><span class="nb">FLOAT</span><span class="w"> </span><span class="k">DEFAULT</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">9</span>
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a><span class="n">gpu_layers</span><span class="w"> </span><span class="nb">INTEGER</span><span class="w"> </span><span class="k">DEFAULT</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="w">      </span><span class="c1">-- -1 = auto, 0 = CPU only (llama.cpp)</span>
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a><span class="n">num_threads</span><span class="w"> </span><span class="nb">INTEGER</span><span class="w"> </span><span class="k">DEFAULT</span><span class="w"> </span><span class="mi">4</span><span class="w">      </span><span class="c1">-- CPU threads (llama.cpp)</span>
</code></pre></div>
<h3 id="deployment-scenarios">Deployment Scenarios</h3>
<h4 id="scenario-1-docker-compose-all-in-one-recommended">Scenario 1: Docker Compose All-in-One (Recommended)</h4>
<p>We provide hardware-optimized docker-compose profiles:</p>
<h5 id="docker-composeollamayml-main-ollama-config">docker-compose.ollama.yml (Main Ollama Config)</h5>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.8&#39;</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="nt">services</span><span class="p">:</span>
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span class="w">  </span><span class="nt">ollama</span><span class="p">:</span>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/ollama:latest</span>
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="w">    </span><span class="nt">container_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kg-ollama</span>
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span class="w">    </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;11434:11434&quot;</span>
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span>
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama-models:/root/.ollama</span>
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a><span class="w">    </span><span class="nt">environment</span><span class="p">:</span>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OLLAMA_NUM_PARALLEL=2</span>
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OLLAMA_MAX_LOADED_MODELS=1</span>
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a><span class="w">    </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">unless-stopped</span>
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a><span class="w">    </span><span class="nt">profiles</span><span class="p">:</span>
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">intel</span>
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">amd</span>
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cpu</span>
<a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a>
<a id="__codelineno-5-21" name="__codelineno-5-21" href="#__codelineno-5-21"></a><span class="w">  </span><span class="c1"># NVIDIA GPU variant</span>
<a id="__codelineno-5-22" name="__codelineno-5-22" href="#__codelineno-5-22"></a><span class="w">  </span><span class="nt">ollama-nvidia</span><span class="p">:</span>
<a id="__codelineno-5-23" name="__codelineno-5-23" href="#__codelineno-5-23"></a><span class="w">    </span><span class="nt">extends</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<a id="__codelineno-5-24" name="__codelineno-5-24" href="#__codelineno-5-24"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/ollama:latest</span>
<a id="__codelineno-5-25" name="__codelineno-5-25" href="#__codelineno-5-25"></a><span class="w">    </span><span class="nt">deploy</span><span class="p">:</span>
<a id="__codelineno-5-26" name="__codelineno-5-26" href="#__codelineno-5-26"></a><span class="w">      </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-5-27" name="__codelineno-5-27" href="#__codelineno-5-27"></a><span class="w">        </span><span class="nt">reservations</span><span class="p">:</span>
<a id="__codelineno-5-28" name="__codelineno-5-28" href="#__codelineno-5-28"></a><span class="w">          </span><span class="nt">devices</span><span class="p">:</span>
<a id="__codelineno-5-29" name="__codelineno-5-29" href="#__codelineno-5-29"></a><span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">driver</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<a id="__codelineno-5-30" name="__codelineno-5-30" href="#__codelineno-5-30"></a><span class="w">              </span><span class="nt">count</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">all</span>
<a id="__codelineno-5-31" name="__codelineno-5-31" href="#__codelineno-5-31"></a><span class="w">              </span><span class="nt">capabilities</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">gpu</span><span class="p p-Indicator">]</span>
<a id="__codelineno-5-32" name="__codelineno-5-32" href="#__codelineno-5-32"></a><span class="w">    </span><span class="nt">profiles</span><span class="p">:</span>
<a id="__codelineno-5-33" name="__codelineno-5-33" href="#__codelineno-5-33"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<a id="__codelineno-5-34" name="__codelineno-5-34" href="#__codelineno-5-34"></a>
<a id="__codelineno-5-35" name="__codelineno-5-35" href="#__codelineno-5-35"></a><span class="w">  </span><span class="c1"># Intel GPU variant (Arc, Iris Xe)</span>
<a id="__codelineno-5-36" name="__codelineno-5-36" href="#__codelineno-5-36"></a><span class="w">  </span><span class="nt">ollama-intel</span><span class="p">:</span>
<a id="__codelineno-5-37" name="__codelineno-5-37" href="#__codelineno-5-37"></a><span class="w">    </span><span class="nt">extends</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<a id="__codelineno-5-38" name="__codelineno-5-38" href="#__codelineno-5-38"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/ollama:latest</span>
<a id="__codelineno-5-39" name="__codelineno-5-39" href="#__codelineno-5-39"></a><span class="w">    </span><span class="nt">devices</span><span class="p">:</span>
<a id="__codelineno-5-40" name="__codelineno-5-40" href="#__codelineno-5-40"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/dev/dri:/dev/dri</span><span class="w">  </span><span class="c1"># Intel GPU device</span>
<a id="__codelineno-5-41" name="__codelineno-5-41" href="#__codelineno-5-41"></a><span class="w">    </span><span class="nt">environment</span><span class="p">:</span>
<a id="__codelineno-5-42" name="__codelineno-5-42" href="#__codelineno-5-42"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OLLAMA_INTEL_GPU=1</span>
<a id="__codelineno-5-43" name="__codelineno-5-43" href="#__codelineno-5-43"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NEOReadDebugKeys=1</span>
<a id="__codelineno-5-44" name="__codelineno-5-44" href="#__codelineno-5-44"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClDeviceGlobalMemSizeAvailablePercent=100</span>
<a id="__codelineno-5-45" name="__codelineno-5-45" href="#__codelineno-5-45"></a><span class="w">    </span><span class="nt">profiles</span><span class="p">:</span>
<a id="__codelineno-5-46" name="__codelineno-5-46" href="#__codelineno-5-46"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">intel</span>
<a id="__codelineno-5-47" name="__codelineno-5-47" href="#__codelineno-5-47"></a>
<a id="__codelineno-5-48" name="__codelineno-5-48" href="#__codelineno-5-48"></a><span class="w">  </span><span class="c1"># AMD GPU variant (ROCm)</span>
<a id="__codelineno-5-49" name="__codelineno-5-49" href="#__codelineno-5-49"></a><span class="w">  </span><span class="nt">ollama-amd</span><span class="p">:</span>
<a id="__codelineno-5-50" name="__codelineno-5-50" href="#__codelineno-5-50"></a><span class="w">    </span><span class="nt">extends</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<a id="__codelineno-5-51" name="__codelineno-5-51" href="#__codelineno-5-51"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/ollama:rocm</span>
<a id="__codelineno-5-52" name="__codelineno-5-52" href="#__codelineno-5-52"></a><span class="w">    </span><span class="nt">devices</span><span class="p">:</span>
<a id="__codelineno-5-53" name="__codelineno-5-53" href="#__codelineno-5-53"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/dev/kfd:/dev/kfd</span>
<a id="__codelineno-5-54" name="__codelineno-5-54" href="#__codelineno-5-54"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/dev/dri:/dev/dri</span>
<a id="__codelineno-5-55" name="__codelineno-5-55" href="#__codelineno-5-55"></a><span class="w">    </span><span class="nt">group_add</span><span class="p">:</span>
<a id="__codelineno-5-56" name="__codelineno-5-56" href="#__codelineno-5-56"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">video</span>
<a id="__codelineno-5-57" name="__codelineno-5-57" href="#__codelineno-5-57"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">render</span>
<a id="__codelineno-5-58" name="__codelineno-5-58" href="#__codelineno-5-58"></a><span class="w">    </span><span class="nt">environment</span><span class="p">:</span>
<a id="__codelineno-5-59" name="__codelineno-5-59" href="#__codelineno-5-59"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HSA_OVERRIDE_GFX_VERSION=11.0.0</span><span class="w">  </span><span class="c1"># Adjust for your AMD GPU</span>
<a id="__codelineno-5-60" name="__codelineno-5-60" href="#__codelineno-5-60"></a><span class="w">    </span><span class="nt">profiles</span><span class="p">:</span>
<a id="__codelineno-5-61" name="__codelineno-5-61" href="#__codelineno-5-61"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">amd</span>
<a id="__codelineno-5-62" name="__codelineno-5-62" href="#__codelineno-5-62"></a>
<a id="__codelineno-5-63" name="__codelineno-5-63" href="#__codelineno-5-63"></a><span class="w">  </span><span class="c1"># CPU-only variant (optimized for AVX2/AVX512)</span>
<a id="__codelineno-5-64" name="__codelineno-5-64" href="#__codelineno-5-64"></a><span class="w">  </span><span class="nt">ollama-cpu</span><span class="p">:</span>
<a id="__codelineno-5-65" name="__codelineno-5-65" href="#__codelineno-5-65"></a><span class="w">    </span><span class="nt">extends</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<a id="__codelineno-5-66" name="__codelineno-5-66" href="#__codelineno-5-66"></a><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama/ollama:latest</span>
<a id="__codelineno-5-67" name="__codelineno-5-67" href="#__codelineno-5-67"></a><span class="w">    </span><span class="nt">environment</span><span class="p">:</span>
<a id="__codelineno-5-68" name="__codelineno-5-68" href="#__codelineno-5-68"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OLLAMA_NUM_THREADS=8</span>
<a id="__codelineno-5-69" name="__codelineno-5-69" href="#__codelineno-5-69"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OLLAMA_USE_MMAP=true</span>
<a id="__codelineno-5-70" name="__codelineno-5-70" href="#__codelineno-5-70"></a><span class="w">    </span><span class="nt">cpus</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<a id="__codelineno-5-71" name="__codelineno-5-71" href="#__codelineno-5-71"></a><span class="w">    </span><span class="nt">mem_limit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16g</span>
<a id="__codelineno-5-72" name="__codelineno-5-72" href="#__codelineno-5-72"></a><span class="w">    </span><span class="nt">profiles</span><span class="p">:</span>
<a id="__codelineno-5-73" name="__codelineno-5-73" href="#__codelineno-5-73"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cpu</span>
<a id="__codelineno-5-74" name="__codelineno-5-74" href="#__codelineno-5-74"></a>
<a id="__codelineno-5-75" name="__codelineno-5-75" href="#__codelineno-5-75"></a><span class="nt">volumes</span><span class="p">:</span>
<a id="__codelineno-5-76" name="__codelineno-5-76" href="#__codelineno-5-76"></a><span class="w">  </span><span class="nt">ollama-models</span><span class="p">:</span>
</code></pre></div>
<h5 id="usage-by-hardware-type">Usage by Hardware Type</h5>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="c1"># NVIDIA GPU (default for most users)</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>docker-compose<span class="w"> </span>-f<span class="w"> </span>docker-compose.yml<span class="w"> </span>-f<span class="w"> </span>docker-compose.ollama.yml<span class="w"> </span>--profile<span class="w"> </span>nvidia<span class="w"> </span>up<span class="w"> </span>-d
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="c1"># Intel GPU (Arc, Iris Xe)</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>docker-compose<span class="w"> </span>-f<span class="w"> </span>docker-compose.yml<span class="w"> </span>-f<span class="w"> </span>docker-compose.ollama.yml<span class="w"> </span>--profile<span class="w"> </span>intel<span class="w"> </span>up<span class="w"> </span>-d
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="c1"># AMD GPU (ROCm-compatible)</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>docker-compose<span class="w"> </span>-f<span class="w"> </span>docker-compose.yml<span class="w"> </span>-f<span class="w"> </span>docker-compose.ollama.yml<span class="w"> </span>--profile<span class="w"> </span>amd<span class="w"> </span>up<span class="w"> </span>-d
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span class="c1"># CPU-only (no GPU)</span>
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>docker-compose<span class="w"> </span>-f<span class="w"> </span>docker-compose.yml<span class="w"> </span>-f<span class="w"> </span>docker-compose.ollama.yml<span class="w"> </span>--profile<span class="w"> </span>cpu<span class="w"> </span>up<span class="w"> </span>-d
</code></pre></div>
<h5 id="main-docker-composeyml-integration">Main docker-compose.yml Integration</h5>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="nt">services</span><span class="p">:</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="w">  </span><span class="nt">api</span><span class="p">:</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="w">    </span><span class="nt">build</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">.</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="w">    </span><span class="nt">depends_on</span><span class="p">:</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">postgres</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span><span class="w">  </span><span class="c1"># Any ollama variant</span>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a><span class="w">    </span><span class="nt">environment</span><span class="p">:</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">EXTRACTION_PROVIDER=ollama</span>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">EXTRACTION_BASE_URL=http://kg-ollama:11434</span>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">EXTRACTION_MODEL=mistral:7b-instruct</span>
</code></pre></div>
<p><strong>Usage:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="c1"># Start everything</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>docker-compose<span class="w"> </span>up<span class="w"> </span>-d
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="c1"># Pull model (first time)</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>ollama<span class="w"> </span>ollama<span class="w"> </span>pull<span class="w"> </span>mistral:7b-instruct
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a><span class="c1"># Configure extraction</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a>kg<span class="w"> </span>admin<span class="w"> </span>extraction<span class="w"> </span><span class="nb">set</span><span class="w"> </span>--provider<span class="w"> </span>ollama<span class="w"> </span>--model<span class="w"> </span>mistral:7b-instruct
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a><span class="c1"># Test</span>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a>kg<span class="w"> </span>ingest<span class="w"> </span>file<span class="w"> </span>-o<span class="w"> </span><span class="s2">&quot;Test&quot;</span><span class="w"> </span>-y<span class="w"> </span>document.txt
</code></pre></div></p>
<h4 id="scenario-2-external-ollama-instance">Scenario 2: External Ollama Instance</h4>
<p>User already has Ollama running elsewhere:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="c1"># Point to existing Ollama</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>kg<span class="w"> </span>admin<span class="w"> </span>extraction<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a><span class="w">  </span>--provider<span class="w"> </span>ollama<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="w">  </span>--model<span class="w"> </span>mistral:7b-instruct<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a><span class="w">  </span>--base-url<span class="w"> </span>http://my-gpu-server:11434
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a><span class="c1"># System connects to external endpoint</span>
</code></pre></div>
<h4 id="scenario-3-system-wide-ollama-installation">Scenario 3: System-Wide Ollama Installation</h4>
<p>User installs Ollama on host machine:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="c1"># Install Ollama</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://ollama.com/install.sh<span class="w"> </span><span class="p">|</span><span class="w"> </span>sh
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a><span class="c1"># Pull model</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>ollama<span class="w"> </span>pull<span class="w"> </span>mistral:7b-instruct
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a><span class="c1"># Configure to use localhost</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>kg<span class="w"> </span>admin<span class="w"> </span>extraction<span class="w"> </span><span class="nb">set</span><span class="w"> </span>--provider<span class="w"> </span>ollama<span class="w"> </span>--model<span class="w"> </span>mistral:7b-instruct
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="c1"># (base_url defaults to http://localhost:11434)</span>
</code></pre></div>
<h4 id="scenario-4-vllm-for-enterprise">Scenario 4: vLLM for Enterprise</h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="c1"># Start vLLM container</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="w">  </span>-p<span class="w"> </span><span class="m">8000</span>:8000<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a><span class="w">  </span>vllm/vllm-openai:latest<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a><span class="w">  </span>--model<span class="w"> </span>meta-llama/Llama-3.1-70B-Instruct
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a>
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="c1"># Configure extraction</span>
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a>kg<span class="w"> </span>admin<span class="w"> </span>extraction<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a><span class="w">  </span>--provider<span class="w"> </span>vllm<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a><span class="w">  </span>--model<span class="w"> </span>meta-llama/Llama-3.1-70B-Instruct<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a><span class="w">  </span>--base-url<span class="w"> </span>http://localhost:8000
</code></pre></div>
<hr />
<h2 id="hardware-deployment-profiles">Hardware Deployment Profiles</h2>
<p>Based on development machine specifications and realistic deployment scenarios:</p>
<h3 id="reference-hardware-development-machine">Reference Hardware (Development Machine)</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>CPU:    AMD Ryzen 9 9950X3D (16 cores, 32 threads)
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>RAM:    123 GB DDR5
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>GPU:    NVIDIA GeForce RTX 4060 Ti (16 GB VRAM)
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>Disk:   1.9 TB NVMe SSD
</code></pre></div>
<h3 id="profile-1-budget-cpu-only-entry-level">Profile 1: Budget CPU-Only (Entry Level)</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>CPU:    Intel i7-12700K or AMD Ryzen 7 5800X (12 cores, 20 threads)
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>RAM:    32 GB DDR4 (recommend 48-64 GB)
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>GPU:    None
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>Disk:   512 GB NVMe SSD
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>Cost:   ~$800-1000
<a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a>Resource Allocation:
<a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a>- PostgreSQL + AGE:      4-8 GB RAM, 2-4 CPU cores
<a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a>- Embedding Model:       2-4 GB RAM, 2 CPU cores (quantized)
<a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a>- Extraction Model:      12-16 GB RAM, 6-8 CPU cores (quantized)
<a id="__codelineno-13-11" name="__codelineno-13-11" href="#__codelineno-13-11"></a>- FastAPI + Workers:     2-4 GB RAM, 2 CPU cores
<a id="__codelineno-13-12" name="__codelineno-13-12" href="#__codelineno-13-12"></a>- System Overhead:       4-6 GB RAM
<a id="__codelineno-13-13" name="__codelineno-13-13" href="#__codelineno-13-13"></a>Total:                   24-38 GB RAM
<a id="__codelineno-13-14" name="__codelineno-13-14" href="#__codelineno-13-14"></a>
<a id="__codelineno-13-15" name="__codelineno-13-15" href="#__codelineno-13-15"></a>Recommended Models:
<a id="__codelineno-13-16" name="__codelineno-13-16" href="#__codelineno-13-16"></a>- Mistral 7B Instruct (Q4_K_M: ~4GB)
<a id="__codelineno-13-17" name="__codelineno-13-17" href="#__codelineno-13-17"></a>- Llama 3.1 8B (Q4_K_M: ~4.5GB)
<a id="__codelineno-13-18" name="__codelineno-13-18" href="#__codelineno-13-18"></a>- Phi-3 Medium 14B (Q4_K_M: ~8GB)
<a id="__codelineno-13-19" name="__codelineno-13-19" href="#__codelineno-13-19"></a>
<a id="__codelineno-13-20" name="__codelineno-13-20" href="#__codelineno-13-20"></a>Performance:
<a id="__codelineno-13-21" name="__codelineno-13-21" href="#__codelineno-13-21"></a>- ~2-5 tokens/second
<a id="__codelineno-13-22" name="__codelineno-13-22" href="#__codelineno-13-22"></a>- ~30-90 seconds per chunk
<a id="__codelineno-13-23" name="__codelineno-13-23" href="#__codelineno-13-23"></a>- ~5-75 minutes per document (10-50 chunks)
<a id="__codelineno-13-24" name="__codelineno-13-24" href="#__codelineno-13-24"></a>- Best for: Personal use, low-volume ingestion, testing
</code></pre></div>
<h3 id="profile-2-mid-range-gpu-prosumer">Profile 2: Mid-Range GPU (Prosumer)</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>CPU:    AMD Ryzen 9 7900X (12 cores, 24 threads)
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>RAM:    64 GB DDR5
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>GPU:    NVIDIA RTX 4070 (12 GB VRAM) or RTX 3060 (12 GB VRAM)
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>Disk:   1 TB NVMe SSD
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a>Cost:   ~$1500-2000
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>
<a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a>Resource Allocation:
<a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a>- PostgreSQL + AGE:      8-12 GB RAM, 3-4 CPU cores
<a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a>- Embedding Model (GPU): 2-3 GB VRAM, 1 GB RAM
<a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a>- Extraction Model (GPU):8-10 GB VRAM, 4-6 GB RAM
<a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a>- FastAPI + Workers:     3-5 GB RAM, 2-3 CPU cores
<a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a>- System Overhead:       6-8 GB RAM
<a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a>Total:                   23-32 GB RAM, 10-13 GB VRAM
<a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a>
<a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a>Recommended Models:
<a id="__codelineno-14-16" name="__codelineno-14-16" href="#__codelineno-14-16"></a>- Mistral 7B Instruct (FP16: ~14GB or 8-bit: ~7GB)
<a id="__codelineno-14-17" name="__codelineno-14-17" href="#__codelineno-14-17"></a>- Llama 3.1 8B Instruct (FP16: ~16GB or 8-bit: ~8GB)
<a id="__codelineno-14-18" name="__codelineno-14-18" href="#__codelineno-14-18"></a>- Qwen2.5 7B Instruct (FP16: ~14GB)
<a id="__codelineno-14-19" name="__codelineno-14-19" href="#__codelineno-14-19"></a>- Mixtral 8x7B (Q4: ~24GB model size, needs CPU offload)
<a id="__codelineno-14-20" name="__codelineno-14-20" href="#__codelineno-14-20"></a>
<a id="__codelineno-14-21" name="__codelineno-14-21" href="#__codelineno-14-21"></a>Performance:
<a id="__codelineno-14-22" name="__codelineno-14-22" href="#__codelineno-14-22"></a>- ~20-40 tokens/second
<a id="__codelineno-14-23" name="__codelineno-14-23" href="#__codelineno-14-23"></a>- ~10-20 seconds per chunk
<a id="__codelineno-14-24" name="__codelineno-14-24" href="#__codelineno-14-24"></a>- ~2-17 minutes per document (10-50 chunks)
<a id="__codelineno-14-25" name="__codelineno-14-25" href="#__codelineno-14-25"></a>- Best for: Small teams, moderate volume, development
</code></pre></div>
<h3 id="profile-3-high-end-gpu-production-reference-machine">Profile 3: High-End GPU (Production - Reference Machine)</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>CPU:    AMD Ryzen 9 9950X3D (16 cores, 32 threads)
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>RAM:    128 GB DDR5
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>GPU:    NVIDIA RTX 4060 Ti (16 GB VRAM) or RTX 4080 (16 GB VRAM)
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>Disk:   2 TB NVMe SSD
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>Cost:   ~$2500-3500
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a>Resource Allocation:
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>- PostgreSQL + AGE:      12-16 GB RAM, 4-6 CPU cores
<a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a>- Embedding Model (GPU): 2-3 GB VRAM, 512 MB RAM
<a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>- Extraction Model (GPU):12-14 GB VRAM, 6-8 GB RAM
<a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a>- FastAPI + Workers:     4-6 GB RAM, 2-3 CPU cores
<a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a>- System Overhead:       8-10 GB RAM
<a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a>Total:                   32-43 GB RAM, 14-17 GB VRAM
<a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a>
<a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a>Recommended Models:
<a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a>- Llama 3.1 8B Instruct (FP16: ~16GB)
<a id="__codelineno-15-17" name="__codelineno-15-17" href="#__codelineno-15-17"></a>- Mistral 7B Instruct (FP16: ~14GB)
<a id="__codelineno-15-18" name="__codelineno-15-18" href="#__codelineno-15-18"></a>- Qwen2.5 7B Instruct (FP16: ~14GB, excellent reasoning)
<a id="__codelineno-15-19" name="__codelineno-15-19" href="#__codelineno-15-19"></a>- Qwen2.5 14B Instruct (8-bit: ~14GB, highest quality)
<a id="__codelineno-15-20" name="__codelineno-15-20" href="#__codelineno-15-20"></a>- Phi-3.5 Mini Instruct (FP16: ~7.6GB, fastest)
<a id="__codelineno-15-21" name="__codelineno-15-21" href="#__codelineno-15-21"></a>- Gemma 2 9B Instruct (8-bit: ~9GB)
<a id="__codelineno-15-22" name="__codelineno-15-22" href="#__codelineno-15-22"></a>
<a id="__codelineno-15-23" name="__codelineno-15-23" href="#__codelineno-15-23"></a>Performance:
<a id="__codelineno-15-24" name="__codelineno-15-24" href="#__codelineno-15-24"></a>- ~30-60 tokens/second (7-8B models)
<a id="__codelineno-15-25" name="__codelineno-15-25" href="#__codelineno-15-25"></a>- ~20-40 tokens/second (14B models)
<a id="__codelineno-15-26" name="__codelineno-15-26" href="#__codelineno-15-26"></a>- ~5-15 seconds per chunk
<a id="__codelineno-15-27" name="__codelineno-15-27" href="#__codelineno-15-27"></a>- ~1-13 minutes per document (10-50 chunks)
<a id="__codelineno-15-28" name="__codelineno-15-28" href="#__codelineno-15-28"></a>- Best for: Production deployments, high-volume ingestion
</code></pre></div>
<h3 id="profile-4-professional-gpu-enterprise">Profile 4: Professional GPU (Enterprise)</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>CPU:    AMD Threadripper PRO 5975WX (32 cores, 64 threads)
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>RAM:    256 GB DDR4 ECC
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>GPU:    NVIDIA RTX 6000 Ada (48 GB VRAM) or A100 (40-80 GB VRAM)
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a>Disk:   4 TB NVMe RAID
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>Cost:   ~$8000-15000
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a>
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>Resource Allocation:
<a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a>- PostgreSQL + AGE:      32-48 GB RAM, 8-12 CPU cores
<a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a>- Embedding Model (GPU): 2-3 GB VRAM, 1 GB RAM
<a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a>- Extraction Model (GPU):40-45 GB VRAM, 12-16 GB RAM
<a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a>- FastAPI + Workers:     8-12 GB RAM, 4-6 CPU cores
<a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>- System Overhead:       16-24 GB RAM
<a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a>Total:                   69-102 GB RAM, 42-48 GB VRAM
<a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a>
<a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a>Recommended Models:
<a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a>- Llama 3.1 70B Instruct (8-bit: ~35GB or 4-bit: ~20GB)
<a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a>- Qwen2.5 72B Instruct (8-bit: ~36GB, best reasoning)
<a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a>- Mixtral 8x22B (Q4: ~42GB)
<a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a>- DeepSeek Coder 33B (8-bit: ~17GB, code-focused)
<a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a>- Hybrid: 70B + 8B routing by complexity
<a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a>
<a id="__codelineno-16-22" name="__codelineno-16-22" href="#__codelineno-16-22"></a>Performance:
<a id="__codelineno-16-23" name="__codelineno-16-23" href="#__codelineno-16-23"></a>- ~40-100 tokens/second (7-8B models)
<a id="__codelineno-16-24" name="__codelineno-16-24" href="#__codelineno-16-24"></a>- ~10-30 tokens/second (70B models)
<a id="__codelineno-16-25" name="__codelineno-16-25" href="#__codelineno-16-25"></a>- ~3-10 seconds per chunk
<a id="__codelineno-16-26" name="__codelineno-16-26" href="#__codelineno-16-26"></a>- ~0.5-8 minutes per document (10-50 chunks)
<a id="__codelineno-16-27" name="__codelineno-16-27" href="#__codelineno-16-27"></a>- Best for: Enterprise, highest quality extraction
</code></pre></div>
<h3 id="profile-5-cloudbare-metal-hyperscale">Profile 5: Cloud/Bare Metal (Hyperscale)</h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>CPU:    Dual EPYC 7763 (128 cores, 256 threads)
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>RAM:    512 GB - 1 TB DDR4 ECC
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a>GPU:    4x NVIDIA A100 (80 GB VRAM each) or 8x A40
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a>Disk:   10+ TB NVMe RAID
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>Cost:   ~$50000-100000
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a>
<a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a>Resource Allocation:
<a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a>- PostgreSQL + AGE:      64-128 GB RAM, 16-24 CPU cores
<a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a>- Embedding Model:       4-6 GB VRAM, 2 GB RAM
<a id="__codelineno-17-10" name="__codelineno-17-10" href="#__codelineno-17-10"></a>- Extraction Models:     2-3 GPUs for parallel 70B models
<a id="__codelineno-17-11" name="__codelineno-17-11" href="#__codelineno-17-11"></a>- Vision Model:          1 GPU @ 20-40GB VRAM
<a id="__codelineno-17-12" name="__codelineno-17-12" href="#__codelineno-17-12"></a>- FastAPI + Workers:     16-32 GB RAM, 8-12 CPU cores
<a id="__codelineno-17-13" name="__codelineno-17-13" href="#__codelineno-17-13"></a>Total:                   128-210 GB RAM, tensor parallelism
<a id="__codelineno-17-14" name="__codelineno-17-14" href="#__codelineno-17-14"></a>
<a id="__codelineno-17-15" name="__codelineno-17-15" href="#__codelineno-17-15"></a>Recommended Deployment:
<a id="__codelineno-17-16" name="__codelineno-17-16" href="#__codelineno-17-16"></a>- vLLM with multiple models:
<a id="__codelineno-17-17" name="__codelineno-17-17" href="#__codelineno-17-17"></a>  - 2x Llama 3.1 70B Instruct (load balanced)
<a id="__codelineno-17-18" name="__codelineno-17-18" href="#__codelineno-17-18"></a>  - 1x Qwen2.5 72B (fallback/comparison)
<a id="__codelineno-17-19" name="__codelineno-17-19" href="#__codelineno-17-19"></a>  - 1x Llama 3.2 Vision 90B (multimodal)
<a id="__codelineno-17-20" name="__codelineno-17-20" href="#__codelineno-17-20"></a>- Model routing:
<a id="__codelineno-17-21" name="__codelineno-17-21" href="#__codelineno-17-21"></a>  - Complexity-based (simple → 8B, complex → 70B)
<a id="__codelineno-17-22" name="__codelineno-17-22" href="#__codelineno-17-22"></a>  - Content-based (code → Qwen/DeepSeek, general → Llama)
<a id="__codelineno-17-23" name="__codelineno-17-23" href="#__codelineno-17-23"></a>  - Real-time load balancing
<a id="__codelineno-17-24" name="__codelineno-17-24" href="#__codelineno-17-24"></a>
<a id="__codelineno-17-25" name="__codelineno-17-25" href="#__codelineno-17-25"></a>Performance:
<a id="__codelineno-17-26" name="__codelineno-17-26" href="#__codelineno-17-26"></a>- ~200+ tokens/second aggregate
<a id="__codelineno-17-27" name="__codelineno-17-27" href="#__codelineno-17-27"></a>- &lt;5 seconds per chunk
<a id="__codelineno-17-28" name="__codelineno-17-28" href="#__codelineno-17-28"></a>- &lt;5 minutes per document (parallel ingestion)
<a id="__codelineno-17-29" name="__codelineno-17-29" href="#__codelineno-17-29"></a>- Best for: Large enterprises, 24/7 production, batch processing
</code></pre></div>
<h3 id="model-size-recommendations-by-profile">Model Size Recommendations by Profile</h3>
<table>
<thead>
<tr>
<th>Profile</th>
<th>VRAM</th>
<th>Best Model Size</th>
<th>Quantization</th>
<th>Expected Quality</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU-Only</td>
<td>0 GB</td>
<td>7B</td>
<td>4-bit (Q4_K_M)</td>
<td>Good (85-90% of GPT-4o)</td>
</tr>
<tr>
<td>Mid-Range</td>
<td>12 GB</td>
<td>7-8B</td>
<td>FP16 or 8-bit</td>
<td>Very Good (90-95% of GPT-4o)</td>
</tr>
<tr>
<td>High-End</td>
<td>16 GB</td>
<td>7-14B</td>
<td>FP16</td>
<td>Excellent (95-98% of GPT-4o)</td>
</tr>
<tr>
<td>Professional</td>
<td>48 GB</td>
<td>70B</td>
<td>8-bit or 4-bit</td>
<td>Near-GPT-4o (98-100%)</td>
</tr>
<tr>
<td>Enterprise</td>
<td>320+ GB</td>
<td>70B+</td>
<td>FP16 or multiple</td>
<td>GPT-4o equivalent or better</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="performance-analysis">Performance Analysis</h2>
<h3 id="document-ingestion-timeline">Document Ingestion Timeline</h3>
<p><strong>Example:</strong> 25,000-word technical document</p>
<table>
<thead>
<tr>
<th>Profile</th>
<th>Model</th>
<th>Chunking</th>
<th>Extraction (25 chunks)</th>
<th>Total</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o API</td>
<td>gpt-4o</td>
<td>2s</td>
<td>50s (parallel)</td>
<td>~52s</td>
<td>$0.25</td>
</tr>
<tr>
<td>Anthropic API</td>
<td>claude-sonnet-4</td>
<td>2s</td>
<td>45s (parallel)</td>
<td>~47s</td>
<td>$0.20</td>
</tr>
<tr>
<td>CPU-Only</td>
<td>Mistral 7B Q4</td>
<td>2s</td>
<td>37min (serial)</td>
<td>~37min</td>
<td>$0</td>
</tr>
<tr>
<td>Mid-Range GPU</td>
<td>Llama 8B FP16</td>
<td>2s</td>
<td>7min (serial)</td>
<td>~7min</td>
<td>$0</td>
</tr>
<tr>
<td>High-End GPU</td>
<td>Qwen 14B 8-bit</td>
<td>2s</td>
<td>4min (serial)</td>
<td>~4min</td>
<td>$0</td>
</tr>
<tr>
<td>Professional</td>
<td>Llama 70B 8-bit</td>
<td>2s</td>
<td>3min (serial)</td>
<td>~3min</td>
<td>$0</td>
</tr>
<tr>
<td>Enterprise</td>
<td>2x 70B parallel</td>
<td>2s</td>
<td>90s (parallel)</td>
<td>~92s</td>
<td>$0</td>
</tr>
</tbody>
</table>
<p><strong>Key Insights:</strong>
- <strong>Cloud APIs:</strong> Fastest for single documents (~1min), but costs scale linearly
- <strong>CPU-Only:</strong> Slow but functional for batch/overnight processing
- <strong>Mid-Range GPU:</strong> Sweet spot for most users (7min acceptable for batch)
- <strong>High-End GPU (16GB):</strong> Production-ready performance (~4min/document)
- <strong>Enterprise:</strong> Approaches cloud speed with parallel processing</p>
<h3 id="batch-ingestion-analysis">Batch Ingestion Analysis</h3>
<p><strong>Scenario:</strong> 100 documents @ 10,000 words each (1000 chunks total)</p>
<table>
<thead>
<tr>
<th>Profile</th>
<th>Model</th>
<th>Total Time</th>
<th>Throughput</th>
<th>Total Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o API</td>
<td>gpt-4o</td>
<td>~17 hours*</td>
<td>6 docs/hour</td>
<td>$100</td>
</tr>
<tr>
<td>Anthropic API</td>
<td>claude-sonnet-4</td>
<td>~15 hours*</td>
<td>7 docs/hour</td>
<td>$80</td>
</tr>
<tr>
<td>CPU-Only</td>
<td>Mistral 7B Q4</td>
<td>~62 hours</td>
<td>2 docs/hour</td>
<td>$0</td>
</tr>
<tr>
<td>Mid-Range GPU</td>
<td>Llama 8B FP16</td>
<td>~12 hours</td>
<td>8 docs/hour</td>
<td>$0</td>
</tr>
<tr>
<td>High-End GPU</td>
<td>Qwen 14B 8-bit</td>
<td>~7 hours</td>
<td>14 docs/hour</td>
<td>$0</td>
</tr>
<tr>
<td>Professional</td>
<td>Llama 70B 8-bit</td>
<td>~5 hours</td>
<td>20 docs/hour</td>
<td>$0</td>
</tr>
<tr>
<td>Enterprise</td>
<td>2x 70B parallel</td>
<td>~2.5 hours</td>
<td>40 docs/hour</td>
<td>$0</td>
</tr>
</tbody>
</table>
<p>*Rate limits and throttling included</p>
<p><strong>Break-Even Analysis:</strong>
- <strong>Mid-Range GPU ($1500):</strong> Pays for itself after ~1,200 documents (vs GPT-4o)
- <strong>High-End GPU ($3000):</strong> Pays for itself after ~2,400 documents
- <strong>No ongoing costs</strong> - only electricity (~$0.10-0.50/hour for GPU)</p>
<hr />
<h2 id="implementation-plan">Implementation Plan</h2>
<h3 id="phase-1-ollama-integration-mvp-week-1-2">Phase 1: Ollama Integration (MVP) - Week 1-2</h3>
<p><strong>Goals:</strong>
- Basic local extraction with Ollama
- Support 7-8B models (Mistral, Llama, Qwen)
- JSON mode for structured output
- Configuration and CLI commands</p>
<p><strong>Tasks:</strong>
1. Create <code>OllamaProvider</code> class extending <code>AIProvider</code>
   - Implement <code>extract_concepts()</code> using Ollama API
   - JSON mode configuration
   - Error handling and retries</p>
<ol>
<li>
<p>Add extraction config fields (migration 007):
   <div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="k">ALTER</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="n">kg_api</span><span class="p">.</span><span class="n">ai_extraction_config</span>
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a><span class="k">ADD</span><span class="w"> </span><span class="k">COLUMN</span><span class="w"> </span><span class="n">backend</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span><span class="w">           </span><span class="c1">-- &quot;ollama&quot;, &quot;vllm&quot;, &quot;openai&quot;, &quot;anthropic&quot;</span>
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a><span class="k">ADD</span><span class="w"> </span><span class="k">COLUMN</span><span class="w"> </span><span class="n">base_url</span><span class="w"> </span><span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span><span class="w">         </span><span class="c1">-- &quot;http://localhost:11434&quot;</span>
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a><span class="k">ADD</span><span class="w"> </span><span class="k">COLUMN</span><span class="w"> </span><span class="n">temperature</span><span class="w"> </span><span class="nb">FLOAT</span><span class="w"> </span><span class="k">DEFAULT</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="c1">-- Low for consistent JSON</span>
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a><span class="k">ADD</span><span class="w"> </span><span class="k">COLUMN</span><span class="w"> </span><span class="n">top_p</span><span class="w"> </span><span class="nb">FLOAT</span><span class="w"> </span><span class="k">DEFAULT</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">9</span><span class="p">,</span>
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a><span class="k">ADD</span><span class="w"> </span><span class="k">COLUMN</span><span class="w"> </span><span class="n">gpu_layers</span><span class="w"> </span><span class="nb">INTEGER</span><span class="w"> </span><span class="k">DEFAULT</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="c1">-- -1 = auto, 0 = CPU only</span>
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a><span class="k">ADD</span><span class="w"> </span><span class="k">COLUMN</span><span class="w"> </span><span class="n">num_threads</span><span class="w"> </span><span class="nb">INTEGER</span><span class="w"> </span><span class="k">DEFAULT</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span><span class="w"> </span><span class="c1">-- CPU threads</span>
</code></pre></div></p>
</li>
<li>
<p>CLI commands:
   <div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>kg<span class="w"> </span>admin<span class="w"> </span>extraction<span class="w"> </span><span class="nb">set</span><span class="w"> </span>--provider<span class="w"> </span><span class="nb">local</span><span class="w"> </span>--backend<span class="w"> </span>ollama<span class="w"> </span>--model<span class="w"> </span>mistral:7b-instruct
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>kg<span class="w"> </span>admin<span class="w"> </span>extraction<span class="w"> </span><span class="nb">test</span><span class="w">  </span><span class="c1"># Test current config</span>
</code></pre></div></p>
</li>
<li>
<p>Ollama installation documentation</p>
</li>
</ol>
<p><strong>Deliverables:</strong>
- Working local extraction with Ollama
- Documentation and user guide
- Performance benchmarks</p>
<h3 id="phase-2-quality-validation-week-2-3">Phase 2: Quality Validation - Week 2-3</h3>
<p><strong>Goals:</strong>
- Validate extraction quality vs GPT-4o
- Test relationship type accuracy
- JSON reliability testing
- Edge case handling</p>
<p><strong>Tasks:</strong>
1. Quality Testing Suite
   - 100 test documents (diverse domains)
   - Compare local vs GPT-4o extraction
   - Relationship type accuracy metrics
   - JSON parsing success rate</p>
<ol>
<li>Benchmarking</li>
<li>Tokens/second across models</li>
<li>Memory usage profiling</li>
<li>CPU vs GPU performance</li>
<li>
<p>Concurrent extraction + embedding</p>
</li>
<li>
<p>Error Handling</p>
</li>
<li>Malformed JSON recovery</li>
<li>Timeout handling</li>
<li>Retry logic</li>
<li>Fallback to cloud if needed</li>
</ol>
<p><strong>Acceptance Criteria:</strong>
- 99%+ valid JSON responses
- 90%+ relationship type accuracy
- 90-95% extraction quality vs GPT-4o baseline</p>
<h3 id="phase-3-advanced-features-week-3-4">Phase 3: Advanced Features - Week 3-4</h3>
<p><strong>Goals:</strong>
- Model switching and hot reload
- Resource optimization
- Hybrid mode (local + cloud fallback)
- Multi-model support</p>
<p><strong>Tasks:</strong>
1. Model Management
   - Hot reload local models
   - Model size detection and warnings
   - Automatic quantization selection</p>
<ol>
<li>Hybrid Mode</li>
<li>Try local first, fallback to cloud on error</li>
<li>Configurable fallback threshold</li>
<li>
<p>Cost tracking for hybrid mode</p>
</li>
<li>
<p>Performance Optimization</p>
</li>
<li>Batch processing for parallel chunks</li>
<li>Model quantization recommendations</li>
<li>Memory usage optimization</li>
</ol>
<p><strong>Deliverables:</strong>
- Production-ready local extraction
- Hybrid cloud/local mode
- Performance tuning guide</p>
<h3 id="phase-4-enterprise-features-future">Phase 4: Enterprise Features (Future)</h3>
<p><strong>Goals:</strong>
- vLLM backend support
- Multi-model deployment
- Vision support (code translation, image description)</p>
<p><strong>Tasks:</strong>
1. vLLM Integration
   - Alternative backend for GPU deployments
   - Tensor parallelism for 70B+ models
   - Load balancing across models</p>
<ol>
<li>Vision Models</li>
<li>Llama 3.2 Vision for <code>describe_image()</code></li>
<li>Multimodal extraction pipeline</li>
<li>
<p>Code diagram understanding</p>
</li>
<li>
<p>Advanced Routing</p>
</li>
<li>Complexity-based model selection</li>
<li>Content-type routing (code → Qwen, general → Llama)</li>
<li>Cost/quality optimization</li>
</ol>
<hr />
<h2 id="consequences">Consequences</h2>
<h3 id="positive">Positive</h3>
<ol>
<li><strong>Self-Hosted Capability</strong></li>
<li>Complete air-gapped deployment possible</li>
<li>No external dependencies for extraction</li>
<li>
<p>Full control over models and versions</p>
</li>
<li>
<p><strong>Cost Reduction</strong></p>
</li>
<li>Zero ongoing API costs after hardware investment</li>
<li>Predictable infrastructure costs</li>
<li>
<p>ROI after ~1,000-2,000 documents</p>
</li>
<li>
<p><strong>Privacy &amp; Compliance</strong></p>
</li>
<li>Sensitive documents never leave premises</li>
<li>HIPAA, GDPR, SOC2 compliant deployments</li>
<li>
<p>No data sharing with third parties</p>
</li>
<li>
<p><strong>Performance</strong></p>
</li>
<li>No network latency</li>
<li>Parallel processing on local hardware</li>
<li>
<p>Batch ingestion without rate limits</p>
</li>
<li>
<p><strong>Flexibility</strong></p>
</li>
<li>Use latest open-source models</li>
<li>Custom fine-tuned models</li>
<li>
<p>Model switching based on workload</p>
</li>
<li>
<p><strong>Hybrid Capability</strong></p>
</li>
<li>Can combine local + cloud</li>
<li>Fallback to cloud for peak loads</li>
<li>Cost optimization per document</li>
</ol>
<h3 id="negative">Negative</h3>
<ol>
<li><strong>Hardware Requirements</strong></li>
<li>Minimum 32GB RAM, ideally 64GB+</li>
<li>GPU strongly recommended for production</li>
<li>
<p>Storage for model files (5-50GB per model)</p>
</li>
<li>
<p><strong>Initial Setup Complexity</strong></p>
</li>
<li>Ollama installation required</li>
<li>Model downloading (one-time, 5-50GB)</li>
<li>
<p>GPU drivers and CUDA (if using GPU)</p>
</li>
<li>
<p><strong>Quality Trade-offs</strong></p>
</li>
<li>7-8B models: 90-95% of GPT-4o quality</li>
<li>14B models: 95-98% of GPT-4o quality</li>
<li>
<p>70B models needed to match GPT-4o</p>
</li>
<li>
<p><strong>Maintenance</strong></p>
</li>
<li>Model updates manual</li>
<li>Monitoring resource usage</li>
<li>
<p>Troubleshooting local inference issues</p>
</li>
<li>
<p><strong>Performance Variability</strong></p>
</li>
<li>Depends heavily on hardware</li>
<li>CPU-only deployments slow (30-90s/chunk)</li>
<li>Concurrent load affects other services</li>
</ol>
<h3 id="risks-mitigation">Risks &amp; Mitigation</h3>
<table>
<thead>
<tr>
<th>Risk</th>
<th>Impact</th>
<th>Mitigation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Poor JSON reliability</td>
<td>High</td>
<td>Extensive testing, retry logic, schema validation</td>
</tr>
<tr>
<td>Relationship type accuracy</td>
<td>High</td>
<td>Quality benchmarking, 70B models for production</td>
</tr>
<tr>
<td>Resource contention</td>
<td>Medium</td>
<td>Resource limits, monitoring, load balancing</td>
</tr>
<tr>
<td>Model availability</td>
<td>Low</td>
<td>Ollama handles downloads, model caching</td>
</tr>
<tr>
<td>User confusion</td>
<td>Medium</td>
<td>Clear documentation, CLI helpers, error messages</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="security-considerations">Security Considerations</h2>
<ol>
<li><strong>Local Model Files</strong></li>
<li>Models stored in <code>~/.ollama/models</code> (Linux/macOS)</li>
<li>Large files (5-50GB) - ensure disk space</li>
<li>
<p>Consider model file integrity checks</p>
</li>
<li>
<p><strong>Network Access</strong></p>
</li>
<li>Ollama API on localhost:11434 by default</li>
<li>Can expose for distributed deployments (add auth)</li>
<li>
<p>Firewall rules for remote access</p>
</li>
<li>
<p><strong>Resource Limits</strong></p>
</li>
<li>Set memory limits to prevent OOM</li>
<li>GPU allocation management</li>
<li>
<p>Process isolation for Ollama</p>
</li>
<li>
<p><strong>Data Privacy</strong></p>
</li>
<li>All inference happens locally</li>
<li>No telemetry by default</li>
<li>Audit logs for extraction requests</li>
</ol>
<hr />
<h2 id="open-questions">Open Questions</h2>
<ol>
<li><strong>Default Model:</strong> Which model should be default? (Mistral 7B vs Llama 8B vs Qwen 7B)</li>
<li><strong>Fallback Strategy:</strong> Auto-fallback to cloud or explicit user choice?</li>
<li><strong>Installation:</strong> Bundle Ollama installer or document separate install?</li>
<li><strong>Vision Support:</strong> Include in Phase 1 or defer to Phase 4?</li>
<li><strong>Quantization:</strong> Auto-detect optimal quantization based on VRAM?</li>
<li><strong>Monitoring:</strong> Built-in performance metrics or rely on external tools?</li>
</ol>
<hr />
<h2 id="success-metrics">Success Metrics</h2>
<h3 id="quality-metrics">Quality Metrics</h3>
<ul>
<li>✅ 99%+ valid JSON responses</li>
<li>✅ 90%+ relationship type accuracy (compared to GPT-4o baseline)</li>
<li>✅ 95%+ concept extraction quality (F1 score vs GPT-4o)</li>
<li>✅ &lt;5% quote extraction errors (exact match failures)</li>
</ul>
<h3 id="performance-metrics">Performance Metrics</h3>
<ul>
<li>✅ &lt; 30 seconds/chunk on mid-range GPU (acceptable for batch)</li>
<li>✅ &lt; 15 seconds/chunk on high-end GPU (production ready)</li>
<li>✅ &lt; 10 minutes for 10,000-word document (end-to-end)</li>
</ul>
<h3 id="adoption-metrics">Adoption Metrics</h3>
<ul>
<li>✅ 50% of users try local inference within 6 months</li>
<li>✅ 25% of production deployments use local inference</li>
<li>✅ Positive user feedback on cost savings</li>
</ul>
<h3 id="reliability-metrics">Reliability Metrics</h3>
<ul>
<li>✅ 99.9% uptime for local inference service</li>
<li>✅ &lt;1% error rate during extraction</li>
<li>✅ Graceful degradation under load</li>
</ul>
<hr />
<h2 id="related-adrs">Related ADRs</h2>
<ul>
<li><strong>ADR-039:</strong> Local Embedding Service - Similar architectural decision for embeddings</li>
<li><strong>ADR-041:</strong> AI Extraction Config - Existing config system this extends</li>
<li><strong>ADR-025:</strong> Dynamic Relationship Vocabulary - Variable prompt size requirement</li>
<li><strong>ADR-040:</strong> Database Schema Migrations - Migration 007 for new config fields</li>
</ul>
<hr />
<h2 id="references">References</h2>
<ul>
<li>Ollama: https://ollama.com</li>
<li>vLLM: https://github.com/vllm-project/vllm</li>
<li>llama.cpp: https://github.com/ggerganov/llama.cpp</li>
<li>HuggingFace TGI: https://github.com/huggingface/text-generation-inference</li>
<li>Llama 3.1 Model Card: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct</li>
<li>Mistral AI: https://mistral.ai/</li>
<li>Qwen 2.5: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct</li>
</ul>
<hr />
<p><strong>Document Status:</strong> Accepted - Implemented
<strong>Author:</strong> System Architects
<strong>Date:</strong> 2025-10-22
<strong>Implemented:</strong> 2025-10-22</p>
<h2 id="implementation-status">Implementation Status</h2>
<p><strong>Phase 1 (MVP) - Completed:</strong>
- ✅ OllamaProvider class extending AIProvider (<code>src/api/lib/ai_providers.py</code>)
- ✅ Database migration 007 for extraction config fields (<code>schema/migrations/007_add_local_extraction_providers.sql</code>)
- ✅ CLI commands for Ollama configuration (<code>client/src/cli/ai-config.ts</code>)
- ✅ Hardware-optimized Docker Compose profiles (<code>docker-compose.ollama.yml</code>)
  - NVIDIA GPU profile
  - AMD GPU (ROCm) profile
  - Intel GPU profile
  - CPU-only profile
- ✅ Startup/stop scripts (<code>scripts/start-ollama.sh</code>, <code>scripts/stop-ollama.sh</code>)</p>
<p><strong>Phase 2-4 (Future):</strong>
- ⏳ Quality validation testing suite
- ⏳ Hybrid cloud/local fallback mode
- ⏳ vLLM backend support
- ⏳ Vision model integration</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.top", "toc.integrate", "search.suggest", "search.highlight", "content.code.copy"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>