"""
Analyze real paths from the knowledge graph using MCP data

This script uses concept IDs and embeddings from MCP queries to perform
gradient analysis on actual reasoning paths.
"""

import numpy as np
from path_analysis import SemanticPathAnalyzer, Concept
import json

# Path from MCP query results:
# Embedding Models â†’ Model Migration â†’ Unified Embedding Regeneration â†’ Bug Fix

# Real concept data from the knowledge graph
concepts_data = [
    {
        "concept_id": "sha256:62dc3_chunk1_9360a498",
        "label": "Embedding Models",
        "grounding": 0.070,
        # Embedding would be fetched from API - using placeholder for now
    },
    {
        "concept_id": "sha256:62dc3_chunk1_45a7faf6",
        "label": "Model Migration",
        "grounding": 0.0,
    },
    {
        "concept_id": "sha256:95454_chunk1_76de0274",
        "label": "Unified Embedding Regeneration",
        "grounding": 0.168,
    },
    {
        "concept_id": "sha256:95454_chunk1_6a25165c",
        "label": "Bug Fix in Source Embedding Regeneration",
        "grounding": 0.0,
    },
]


def fetch_embedding_from_api(concept_id: str) -> np.ndarray:
    """
    Fetch embedding from API

    In production, this would call:
    GET /queries/concepts/{concept_id}

    For now, simulate with random embeddings of correct dimension
    """
    # TODO: Replace with actual API call
    # response = requests.get(f"http://localhost:8000/queries/concepts/{concept_id}")
    # return np.array(response.json()['embedding'], dtype=np.float32)

    # Simulated embedding (768 dimensions like nomic-embed)
    # In reality these would be the actual embeddings from the database
    np.random.seed(hash(concept_id) % (2**32))  # Deterministic based on ID
    return np.random.randn(768).astype(np.float32)


def analyze_path(concepts_data: list):
    """Analyze gradient properties of a real graph path"""

    print("=" * 70)
    print("Semantic Path Gradient Analysis - Real Knowledge Graph Path")
    print("=" * 70)

    # Build Concept objects with embeddings
    print("\nğŸ“¥ Fetching embeddings for concepts...")
    concepts = []
    for c_data in concepts_data:
        embedding = fetch_embedding_from_api(c_data['concept_id'])
        concepts.append(Concept(
            concept_id=c_data['concept_id'],
            label=c_data['label'],
            embedding=embedding,
            grounding=c_data.get('grounding', 0.0)
        ))
        print(f"  âœ“ {c_data['label']}")

    # Analyze path
    print("\nğŸ“Š Analyzing path with gradient-based metrics...")
    analyzer = SemanticPathAnalyzer(weak_link_threshold=2.0)
    metrics = analyzer.analyze_path(concepts)

    # Display results
    print("\n" + "=" * 70)
    print("RESULTS")
    print("=" * 70)

    print(f"\nğŸ›¤ï¸  Reasoning Path:")
    path_str = " â†’ ".join(c.label for c in concepts)
    print(f"  {path_str}")

    print(f"\nğŸ“ Distance Metrics:")
    print(f"  Total Distance: {metrics.total_distance:.4f}")
    print(f"  Average Step Size: {metrics.avg_step_size:.4f}")
    print(f"  Step Variance: {metrics.step_variance:.6f}")

    print(f"\nğŸ¯ Coherence Analysis:")
    print(f"  Coherence Score: {metrics.coherence_score:.4f}")
    coherence_rating = (
        "Excellent" if metrics.coherence_score > 0.9
        else "Good" if metrics.coherence_score > 0.7
        else "Moderate" if metrics.coherence_score > 0.5
        else "Poor"
    )
    print(f"  Coherence Rating: {coherence_rating}")
    print(f"  Overall Quality: {metrics.quality_rating}")

    print(f"\nğŸŒ€ Curvature Analysis:")
    if metrics.curvature_angles:
        print(f"  Average Curvature: {metrics.avg_curvature:.4f} radians ({np.degrees(metrics.avg_curvature):.1f}Â°)")
        print(f"  Min Curvature: {min(metrics.curvature_angles):.4f} rad")
        print(f"  Max Curvature: {max(metrics.curvature_angles):.4f} rad")

        # Interpret curvature
        if metrics.avg_curvature < 0.5:
            print(f"  Interpretation: âœ“ Smooth progression (low curvature)")
        elif metrics.avg_curvature < 1.5:
            print(f"  Interpretation: â— Moderate turns (medium curvature)")
        else:
            print(f"  Interpretation: â—¯ Sharp pivots (high curvature)")
    else:
        print(f"  (Need 3+ concepts for curvature analysis)")

    # Individual steps
    print(f"\nğŸ“ Individual Step Analysis:")
    for i in range(len(concepts) - 1):
        step_gradient = analyzer.semantic_gradient(
            concepts[i].embedding,
            concepts[i+1].embedding
        )
        step_size = analyzer.gradient_magnitude(step_gradient)

        deviation = abs(step_size - metrics.avg_step_size) / (metrics.avg_step_size + 1e-8)

        print(f"\n  Step {i+1}: {concepts[i].label}")
        print(f"          â†’ {concepts[i+1].label}")
        print(f"    Distance: {step_size:.4f}", end="")

        if deviation > 0.5:
            print(f" âš ï¸  ({deviation*100:.0f}% deviation from average)")
        else:
            print(f" âœ“ (within normal range)")

        # Show grounding for context
        print(f"    Source grounding: {concepts[i].grounding:.3f}")
        print(f"    Target grounding: {concepts[i+1].grounding:.3f}")

    # Weak links
    if metrics.weak_links:
        print(f"\nâš ï¸  Weak Links Detected ({len(metrics.weak_links)}):")
        for link in metrics.weak_links:
            print(f"\n  Step {link['step_index'] + 1}: {link['source']} â†’ {link['target']}")
            print(f"    Semantic Gap: {link['distance']:.4f}")
            print(f"    Severity: {link['severity_sigma']:.2f}Ïƒ above mean")
            print(f"    Recommendation: Consider adding intermediate concept")
    else:
        print(f"\nâœ… No weak links detected - path is coherent!")

    # Correlation with grounding
    print(f"\nğŸ”¬ Grounding Correlation Analysis:")
    groundings = [c.grounding for c in concepts]
    avg_grounding = np.mean(groundings)
    print(f"  Average grounding: {avg_grounding:.3f}")

    if avg_grounding < 0.1:
        print(f"  Note: Low grounding - concepts may need more evidence")

    print("\n" + "=" * 70)
    print()

    return metrics


def analyze_semantic_momentum():
    """Demonstrate semantic momentum prediction"""

    print("\n" + "=" * 70)
    print("Semantic Momentum Analysis")
    print("=" * 70)

    # Build shorter path for momentum analysis
    path = [
        Concept(
            concept_id="sha256:62dc3_chunk1_9360a498",
            label="Embedding Models",
            embedding=fetch_embedding_from_api("sha256:62dc3_chunk1_9360a498"),
            grounding=0.070
        ),
        Concept(
            concept_id="sha256:62dc3_chunk1_45a7faf6",
            label="Model Migration",
            embedding=fetch_embedding_from_api("sha256:62dc3_chunk1_45a7faf6"),
            grounding=0.0
        ),
        Concept(
            concept_id="sha256:95454_chunk1_76de0274",
            label="Unified Embedding Regeneration",
            embedding=fetch_embedding_from_api("sha256:95454_chunk1_76de0274"),
            grounding=0.168
        ),
    ]

    # Candidate next concepts (from MCP query Distance 3)
    candidates = [
        Concept(
            concept_id="sha256:95454_chunk1_6a25165c",
            label="Bug Fix in Source Embedding Regeneration",
            embedding=fetch_embedding_from_api("sha256:95454_chunk1_6a25165c"),
        ),
        Concept(
            concept_id="sha256:95454_chunk1_6cf7348c",
            label="Testing and Verification",
            embedding=fetch_embedding_from_api("sha256:95454_chunk1_6cf7348c"),
        ),
        Concept(
            concept_id="sha256:95454_chunk1_1f44c138",
            label="GraphQueryFacade",
            embedding=fetch_embedding_from_api("sha256:95454_chunk1_1f44c138"),
        ),
    ]

    analyzer = SemanticPathAnalyzer()

    print(f"\nğŸ›¤ï¸  Established path:")
    print(f"  {' â†’ '.join(c.label for c in path)}")

    print(f"\nğŸ¯ Checking alignment with semantic momentum:")

    alignments = []
    for candidate in candidates:
        momentum, alignment = analyzer.calculate_semantic_momentum(path, candidate)
        alignments.append((candidate.label, alignment))

        status = "âœ“" if alignment > 0 else "â—¯"
        print(f"  {status} {candidate.label}: {alignment:.4f}")

    # Find best alignment
    best = max(alignments, key=lambda x: x[1])
    print(f"\nâœ¨ Most aligned with path momentum: {best[0]} ({best[1]:.4f})")
    print()


def main():
    """Run gradient analysis on real knowledge graph path"""

    print("\n")
    print("â•”" + "=" * 68 + "â•—")
    print("â•‘" + " " * 10 + "Semantic Path Gradient Analysis" + " " * 27 + "â•‘")
    print("â•‘" + " " * 15 + "Real Knowledge Graph Data" + " " * 28 + "â•‘")
    print("â•š" + "=" * 68 + "â•")
    print()

    # Analyze main path
    metrics = analyze_path(concepts_data)

    # Analyze semantic momentum
    analyze_semantic_momentum()

    print("\nğŸ’¡ Key Insights:")
    print("  â€¢ Gradient-based analysis reveals semantic coherence of reasoning paths")
    print("  â€¢ Weak links indicate where intermediate concepts may be needed")
    print("  â€¢ Curvature shows how sharply concepts pivot in semantic space")
    print("  â€¢ Momentum prediction helps identify logically-aligned next concepts")
    print()
    print("ğŸ“ Next Steps:")
    print("  â€¢ Test correlation between semantic gap and grounding scores")
    print("  â€¢ Analyze multiple paths to establish baseline metrics")
    print("  â€¢ Integrate weak link detection into relationship extraction")
    print()


if __name__ == "__main__":
    main()
